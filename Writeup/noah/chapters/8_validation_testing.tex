\chapter{Post-Development Validation Testing}
\label{chapter:postdevvalidationtesting}
\section{Methodology}
I divided my post-development validation testing into three broad sections, listed below. These were administered to five test subjects.

\subsection{Effort Testing via TLX}
LandFill adds value if it is able to create, with a user's input, a manabase for a deck that they consider to be of equivalent or superior quality to a manabase they could produce with an equal or lesser amount of time and effort. The NASA Task Load Index (TLX) is a questionnaire used widely in industry to measure effort, time and performance. It uses six questions, each ranked out of 100 in increments of 5 to determine a subject's perception of how much effort they expended in pursuit of a task~\cite{bustamante2008measurement}. The questions are as follows~\cite{nasatlxhomepage}:

\begin{enumerate}
\item How mentally demanding was the task?
\item How physically demanding was the task?
\item How hurried or rushed was the pace of the task?
\item How successful were you in accomplishing what you were asked to do? (note that for this question, a score of 0 denotes a feeling of perfect success, not total failure)
\item How hard did you have to work to achieve your level of performance?
\item How insecure, dsicouraged, irritated, stressed and annoyed were you?
\end{enumerate}

A total score can be obtained by summing these scores and divided by 6, although the ``raw'' scores can be valuable as well.

I provided subjects with two incomplete Commander decks, a BUG deck and a WUR deck, both of which consisted solely of 62 nonland cards, and asked them to choose 38 land cards for each. Participants were allowed use of any existing deck-building apps, including Archidekt and Mana Gathering, for the WUR deck, and were asked to complete the BUG deck using LandFill. I timed participants in both tasks. After each task, subjects completed a TLX. This allowed me to compare a subject's perceived effort when building a manabase by hand to their perceived effort when using LandFill. Since this is a within-subjects analysis, each participant functions as their own control, and statistical significance can be determined using a Wilcoxon Signed Rank test (I used the Wilcoxon calculator on Statology.org \cite{statology}). The ``carryover effect' that must normally be accounted for in a within-subjects analysis \cite{withinsubjects} is here irrelevant, as both exercises are meaningfully different.

Although in some TLX analyses the user is asked to weight the six dimensions by significance, there is evidence that this negatively impacts accuracy~\cite{bustamante2008measurement}. As I am interested in comparing raw scores as well, I am not necessarily interested in weighting anyway, so did not do this.

\subsection{Serendipity Testing}
\label{sec:serendipity}
It is impractical to test the quality of manabases recommended by LandFill by simply judging the win/loss rate of decks built around its suggestions. MTG games take time, and introduce many confounding variables, including player skill. Instead, it is more feasible to test simply whether a user is satisfied with the manabase recommended. This is touched on in the TLX, which asks users to rate their ``performance'' at the task (in this case, building a good manabase with LandFill), but warrants dedicated investigation. Problematically, however, several subjects during User Research expressed a desire for an app that would recommend them lands that they might not have thought of otherwise (see \secref{sec:personas}). Therefore, it is important to test the degree to which, insofar as LandFill is making distinct decisions to a human player, those decisions are welcomed by the human player. 

I assessed this via the following method. After users had completed both TLX exercises, I used the preferences they had set for LandFill in generation of the BUG deck to generate an alternative list of lands for the WUR deck, and used a simple Python script to categorize lands (here just string representations of the land name) into:

\begin{itemize}
\item Lands included in both decklists.
\item Lands selected by the user not selected by LandFill.
\item Lands selected by LandFill not selected by the User.
\end{itemize}

For all lands in the final two lists, asked the user's opinion on LandFill's decision to include or exclude each, grouping into cycles where possible for simplicity. Basic Lands were included for the script with their numbers~---~i.e.,~if a player chose 4 Basic Islands, and LandFill chose 6 Basic Islands, than the second list would include ``4 Basic Islands'' and the latter would include ``6 Basic Islands.'' 

While this does not map cleanly onto any standardized testing mechanism, it is inspired by the concept of ``Serendipity'', which is used in the analysis of recommender systems such as those on Spotify and Instagram. Serendipity compares the ``unexpectedness'' of recommended items~---~the similarity between recommended items and a user's previous item interactions~---~with their ``relevance'', i.e.,~whether the user interacted with them after recommendation~\cite{SerendipityRecSys}. However, as these more formalized metrics are designed for large platforms that accrue preference data over time, I significantly adapted them. I also treated them as a prompt for discussion within the below semi-structured interview, and thus a source of qualitative information, rather than as a numerical metric.

\subsection{Semi-Structured Interview}
In addition to the discussion prompts provided by the Serendipity Testing, I asked users if they had any suggestions for improvements or new features, and if they had any general thoughts on LandFill.

I utilized the same lightweight version of Thematic Analysis as I outlined in \secref{sec:userresearchanalysis} and should here note my own bias as the developer. I conducted this research hoping to receive positive feedback. 

\section{Results}
\subsection{TLX Data Analysis}

\myfig{TLX_scores.jpg}{Scores and timings from the two TLX analyses, and the differences between these values.}{tlxdata}

Scores from the TLX questionnaire are broken down by participant in Fig.~\ref{fig:tlxdata}. In all cases, use of LandFill resulted in:

\begin{itemize}
\item A lesser overall perceived workload.
\item An equal or higher satisfaction with performance of the task~---~i.e.,~with the quality of the manabase generated.
\item A reduction in time to complete. 
\end{itemize}

Only participants 4 and 5 felt LandFill required more effort in any of the dimensions, and these were also the participants who felt that LandFill had increased their performance by the highest margin. Only participant 3 recorded no difference between their performance with LandFill and without it. This participant also saved by far the most time through use of LandFill, suggesting that this user naturally takes more time to build decks to a higher standard. According to the Wilcoxon Signed-Rank test, the overall score is statistically significant for an alpha value of 0.1. From running the Wilcoxon Signed-Rank test on the raw scores, I determined that the only raw scores that are not significant within this alpha value are mental demand and frustration. This strongly vindicates my thesis that LandFill is a useful tool for deckbuilders. 

\subsection{Serendipity Test Analysis}
Predictably, LandFill always recommended different quantities of each Basic Land than users. Since the deck under test was WUR, each generated manabase included a number of Basic Island, Basic Plains and Basic Mountain cards. Users said that, when choosing a manabase, they had either divided land slots equally among these after choosing nonbasic lands, or intuited the number based on the proportions required by the deck. All save one were happy to accept LandFill's estimates over their own. Because the test decks are stored on TappedOut, which lists in a pie-chart the proportions of colours required by the deck, it is also notable that even the more thoughtful players here were making decisions based on more information than they may have had access to when deckbuilding normally. In initial user research, many participants said they determine complete decklists, including manabases, before uploading them to TappedOut or another database.

The one subject who \textit{did} favour their own quantity of basics over LandFill used far fewer basics than LandFill recommended. This is because, while users were asked not to consider utility lands, in practice, the definition of a utility land is vague; some utility lands do generate coloured mana. This particular subject chose single-colour utility lands that entered untapped over basic lands, and said that this was a general preference of theirs. 

If a nonbasic land was included in LandFill's list but not in the player's, the player when asked cited one of several possible explanations, including: 

\begin{itemize}
\item The player had forgotten about this land.
\item The player habitually did not think about this land.
\item This land cycle incurs a life point penalty that the player typically shies away from.
\end{itemize}

Only the third case reflects a negative assessment of LandFill's suggestion; in these cases, the player did not use the option to dismiss these lands from consideration. Given the absence of a holistic score assessing the life point damage incurred from the manabase, it may be necessary to include a more visible option to exclude lands based on life point investment in future versions.

If a nonbasic land was included in the player's list but not LandFill's, this was usually because it was a land that was not coded into LandFill. Typically, this was because it was a land that is not contained in a cycle. Currently, the only non-cycle land supported by LandFill is Command Tower. Future iterations would thus likely benefit from inclusion of more non-cycle lands. While many of these lands would behave simply in the Simulator, it is potentially more complicated to introduce these to the frontend, the layout of which is based on cycles. 

\subsection{Semi Structured Interview Response Analysis}
Since much of the interview focussed on potential new features and fixes, the bulk of the data obtained here will be covered shortly in \secref{sec:areasforimprovement}. However, I will briefly touch here on general thoughts. While social desirability bias is a factor here, four out of the five participants described LandFill as a useful service. Subjects highlighted its relevance not only to card selection but to formatting and card recollection, as it prevented spelling mistakes and did not accidentally suggest lands in the wrong colours. Several subjects commented that even if LandFill had not saved them time overall, automation of the process made it feel much less temporally demanding. Compared to the initial mockup, subjects found it much easier to mark cards for inclusion and exclusion, as LandFill now informs them of the behaviour of each cycle via the displayed card image. 

Users identified some bugs. The InputParser did not account for formatting differences between search engines, and the toggle switch to include or remove nonland cards from the final deck output behaved temperamentally. Additionally, no user tested made use of the ability to rank mechanically equivalent lands; players either removed all (or all save one) rankable cycles from consideration or did not put much thought into which lands they would prefer. This feature may have been unnecessary.

In several cases, it was clear in the final output that the results were only approximate. One user noted that one individual Basic Mountain performed far higher in the output than all others did, and higher than several dual lands. A second noted that, in the BUG deck, an ``on-colour'' Fetch Land, searching for an Island or a Forest, ranked below two ``off-colour'' fetches, which could only search for one basic landtype in the deck's colours. These should be considered strictly inferior in context to on-colour fetches. In both these specific cases, this can be addressed without fundamental alteration of LandFill's methods. Regarding the latter, off-colour Fetch Lands can be ranked below on-colour Fetch Lands in the LandPrioritization. Regarding the former, while it is useful during each Hill Climb increment to score each Basic Land separately, as it prevents nonbasic lands from being ejected prematurely, it may be wise to normalize all basics of each colour in the final output. However, in a general sense, it is likely worthwhile to stress to users more clearly that LandFill's recommendation is not absolute.

\section{Additional Features}
\label{sec:areasforimprovement}
\subsection{Persistent Preference Storage and Links with Existing Databases}
\label{sec:persistantstorage}
Two participants suggested that LandFill should store user data. In both cases, the subject felt it was important not simply to persist user preferences, but fully track the lands in a user's collection. This could be done either by allowing users to create a secure account on LandFill to store a list of owned cards, or by allowing LandFill to link with a user's account on another card library database. 

\subsection{Flexible Runtimes and Accuracy}
Several users commented that, as the bulk of LandFill's runtime required no user input, it could afford to take longer than it currently did. Participant 3, notably, spent almost twice as much time producing their manabase as LandFill spent to conduct 10000 simulated games and 2000 card tests per run as detailed in \secref{sec:eachincrement}, which resulted in an increased consistency of 10 percentage points compared to the amount of simulations currently run. They outlined a desirable use-case for LandFill where they specified a high degree of optimization consistency, and then ``left it running while [they] had [their] lunch''. 

In the future, LandFill could offer users one of several accuracy levels, with projected runtimes for each. This would require significant testing in its own right, however, as in order to let users make an informed choice, I would need to analyse the average Jaccard index improvement as a function of runtime increase across 2, 3, 4 and 5 colour decks. 

\subsection{Additional Formats and MTG: Arena Support}
Users suggested the inclusion of other formats as a potential next step. Players also suggested that LandFill be formatted for use with MTG: Arena. 

Given that Commander was the most popular format among my initial test subjects, and one designed for casual play, I stand by my decision to restrict initial development to this format, as it allowed me to focus on the core functionality of LandFill without worrying about handling format specifications. Nevertheless, this is a natural progression in future iterations.

\subsection{Frontend Formatting Improvements}
Participants identified several usability problems. These are listed below.

\begin{itemize}
\item Since the ability to select or deselect individual lands within a cycle was accessible on the side panel via mouseover, players had to move their mouse carefully from the cycle label to the panel to avoid accidentally brushing over another cycle. Ideally this would be replaced by a drop-down menu accessible over the cycle label itself.
\item Currency is set globally on the first page. Subjects requested the ability to change it on subsequent pages, if they missed it on the first one. 
\item Card prices were taken from ScryFall as downloaded by Scrython. One subject pointed out, however, that this meant that LandFill only displays the lowest price for a given land, which may be an undesirable printing. Since cards are sold on a second-hand market, and price differs by printing, prices should be given as a range, not as a single value. 
\item Although a FAQ on the Preferences page outlines the performance metric used, this should be re-stated on the output page in case the user ignored it. 
\item One subject pointed out that, if a cycle is moved to the excluded column, but some individual lands were ticked within it, the label of the cycle should be represented differently within the column to show that it is not entirely excluded.
\end{itemize}


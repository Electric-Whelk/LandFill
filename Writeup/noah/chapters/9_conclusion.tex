\chapter{Conclusion}
\section{Summary}
In this thesis, I have outlined why spontaneous generation of an optimized manabase is complicated. I broadly outlined LandFill's use case, as a flexible adaptation of the testing scripts developed by Frank Karsten so that they could be used to automate card selection for an individual deck, rather than just determine deckbuilding best-practices. I then illustrated how I organized a local database of MTG cards, and equipped it to be continously updated in response to future sets. 

Equipped with this database, I have outlined how I used Python to implement an extremely stripped down MTG simulator, how to encode behaviour that effectively utilizes mana, how to model the behaviour of land cycles, and the heuristics accepted here during initial development. I then demonstrated how to use the output of these simulations in a Hill Climbing algorithm to explore the search space of possible manabases in a way that balances runtime with accuracy. I then illustrated the frontend design that would allow this apparatus to be used by lay MTG players, and how it encourages them to simplify the job of the Optimizer component by specifying lands they do and do not want. 

I finally outlined preliminary test results from this system, and how they strongly suggest that LandFill, subject to de-bugging, is already capable of streamlining the deckbuilding process, making it quicker and easier to assemble manabases to a higher standard. I then outlined features that users proposed, with some examination of how they might be implemented. 

Throughout this writeup, I have highlighted where LandFill is a limited product. Its optimizations, even aside from the susceptibility of Hill Climbing algorithms to local maxima, are approximate and not wholly consistent, and there are subtleties of gameplay that it simplifies even within its simplified goals (maximize mana expenditure) as an automated player. While these do not impede its utility, as user testing demonstrates, I will conclude here by exploring some areas of development where, with hindsight, I may have made different decisions.

\section{Self Assessment}
\subsection{Use of Python as Backend}
\label{sec:python}
Recall from \secref{sec:eachincrement} that, measured by average Jaccard Index, optimizations that took less than ten minutes for a three-colour deck only shared around 70\% of lands in common. This suggests that, with use of the simulator as outlined here, a single optimum~---~even a local maximum~---~cannot be reliably reached in a usable timeframe, although user testing has expanded my perception of what a usable timeframe may be. However, the increase in Jaccard Index when longer timeframes were permitted does suggest that there is room to improve LandFill simply by increasing the speed of the Simulator.

Code profiling was conducted repeatedly through development via the \texttt{line\allowbreak\_profiler} library. It identified the main contributor to the Simulator's runtime the generation of Lumps and the assessment of Lump playability; while I adapted both processes several times to reduce this, and experimented with higher-performance libraries such as SciPy, there remains a runtime floor. Because of this, Python may not have been the best choice of a backend language. I chose Python due to its wide use in webdesign, but it is not considered a high performance language. With hindsight, it may have been wise to explore other options. 

An intermediate solution may be to refactor around a piplining library such as Joblib~\cite{joblib} to allow multiple games to be run simultaneously. While I did explore a joblib-based implementation, the required deepcopying of deck and card objects immediately counteracted any runtime improvements. If it is feasible, it would have had to inform the structure of the objects at a much earlier stage of development. 

\subsection{Use of Hill Climbing Algorithm}
Given the extent of research necessary to determine an appropriate objective function, halting criterion and simulation count per increment for the optimizer, one area did remain underexplored~---~the choice of Steepest Ascent Hill Climbing itself. While I did touch on other Neighbourhood Search variations, one possibility that I did not have time to explore was the use of an Evolutionary Algorithm. In such an algorithm, pairs of high-performing manabases would be randomly shuffled together with small mutations at each increment~\cite{bjorke2017deckbuilding}. Since I did not trial this algorithm, I do not know if it would have been able to reach more consistent results from an equivalent number of simulated games. This would be a potential route for experimentation in future versions. 

\subsection{Mid-Development Validation and Verification Tests}
Although only one participant in my post-development validation tests openly asked for a slower and more accurate product, all users worked more slowly than LandFill. In retrospect, I would have benefitted from a mid-development test after developing the Simulator, where users simply created a manabase on a stopwatch, as this would have enabled me to make more informed decisions about what constitutes a reasonable time for the optimizer to run.

I also did not conduct sufficient verification tests of the Simulator. During development I tested each new land cycle with many sample hands and checked their behaviour, including in combination with other cycles. However, I did not formally Unit Test the Simulator. Unit Testing the Simulator is complicated, as the gameplay decisions of each sample hand play out over many turns. Moreover, as the Simulator chooses lands to play by progressively shrinking a list of possible lands, there is a stochastic element involved if mure than one land is returned by the end of this process, although there are methods to test code with stochastic outputs. Unit testing the Simulator would be an essential test before full deployment, to ensure that existing code is not broken by the introduction of new cycles.

\section{Final Thoughts}
LandFill is a very promising deckbuilder's tool with potential market applicability. Ultimately, it is more successful as an automatic manabase generator for casual use than a comprehensive optimizer, but this is still a valid use case. It may be immediately improved via more thorough Unit Testing and debugging, and implementing more complex simulation code to avoid some of the heuristics outlined in \secref{sec:simulationheuristics}. Broader improvements may be achieved by finding ways to let it conduct more simulated games, either by making the Simulator faster, finding a more efficient Combinatorial Optimization algorithmm or by offering users more input over how long they are content to wait for it. It is, in sum, a successful prototype for an ambitious deckbuilder support app. 
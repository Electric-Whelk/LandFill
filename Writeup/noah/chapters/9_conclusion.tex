\chapter{Conclusion}
\section{Summary}
In this thesis, I have outlined the difficulties in manabase optimization, and demonstrated a means of adapting Frank Karsten's Monte Carlo experiments into a general-purpose deckbuilding app. I illustrated how I assembled the requisite database of MTG cards, and used this to implement a stripped-down MTG simulator capable of efficient mana allocation. I then illustrated how to utilize this in a Hill Climbing algorithm to explore the search space of possible manabases, and a frontend design that would allow use of this system by a player. I concluded by outlining preliminary test results from this system, and how they strongly suggest that LandFill already allows users to assemble manabases more quickly and to a higher standard, as well as what areas they identified for future improvement.

Throughout this thesis, I have highlighted where LandFill is a limited product. Its optimizations, even aside from the susceptibility of Hill Climbing algorithms to local maxima, are approximate and not wholly consistent, and there are subtleties of gameplay that it simplifies even within its simplified goals (maximize mana expenditure) as an automated player. While these do not impede its utility, as user testing demonstrates, I will conclude here by exploring some areas of development where, with hindsight, I may have made different decisions.

\section{Self Assessment}
\subsection{Use of Python as Backend}
\label{sec:python}
Recall from \secref{sec:eachincrement} that, measured by average Jaccard Index, optimizations that took less than ten minutes for a three-colour deck only shared around 70\% of lands in common. This suggests that, with use of the simulator as outlined here, a single optimum~---~even a local maximum~---~cannot be reliably reached in a usable timeframe, although user testing has expanded my perception of what a usable timeframe may be. However, the increase in Jaccard Index when longer timeframes were permitted does suggest that there is room to improve LandFill simply by increasing the speed of the Simulator.

Code profiling was conducted repeatedly through development via the \texttt{line\allowbreak\_profiler} library. It identified the main contributor to the Simulator's runtime the generation of Lumps and the assessment of Lump playability; while I adapted both processes several times to reduce this, and experimented with higher-performance libraries such as SciPy, there remains a runtime floor. Because of this, Python may not have been the best choice of a backend language. I chose Python due to its wide use in webdesign, but it is not considered a high performance language. With hindsight, it may have been wise to explore other options. 

An intermediate solution may be to refactor around a piplining library such as Joblib~\cite{joblib} to allow multiple games to be run simultaneously. While I did explore a joblib-based implementation, the required deepcopying of deck and card objects immediately counteracted any runtime improvements. If it is feasible, it would have had to inform the structure of the objects at a much earlier stage of development. 

\subsection{Use of Hill Climbing Algorithm}
Given the extent of research necessary to determine an appropriate objective function, halting criterion and simulation count per increment for the optimizer, one area did remain underexplored~---~the choice of Steepest Ascent Hill Climbing itself. While I did touch on other Neighbourhood Search variations, one possibility that I did not have time to explore was the use of an Evolutionary Algorithm. In such an algorithm, pairs of high-performing manabases would be randomly shuffled together with small mutations at each increment~\cite{bjorke2017deckbuilding}. Since I did not trial this algorithm, I do not know if it would have been able to reach more consistent results from an equivalent number of simulated games. This would be a potential route for experimentation in future versions. 

\subsection{Mid-Development Validation and Verification Tests}
Although only one participant in my post-development validation tests openly asked for a slower and more accurate product, all users worked more slowly than LandFill. In retrospect, I would have benefitted from a mid-development test after developing the Simulator, where users simply created a manabase on a stopwatch, as this would have enabled me to make more informed decisions about what constitutes a reasonable time for the optimizer to run.

I also did not conduct sufficient verification tests of the Simulator. During development, I tested each new land cycle with many sample hands and checked their behaviour, including in combination with other cycles. However, I did not formally Unit Test the Simulator. Unit Testing the Simulator is complicated, as the gameplay decisions of each sample hand play out over many turns. Moreover, as the Simulator chooses lands to play by progressively shrinking a list of possible lands, there is a stochastic element involved if more than one land is returned by the end of this process, although there are methods to test code with stochastic outputs. Unit testing the Simulator would be an essential test before full deployment, to ensure that existing code is not broken by the introduction of new cycles.

\section{Final Thoughts}
LandFill is a very promising deckbuilder's tool with potential market applicability. Ultimately, it is more successful as an automatic manabase generator for casual use than a comprehensive optimizer, but this is still a valid use case. It may be immediately improved via more thorough Unit Testing and debugging, and implementing more complex simulation code to avoid some of the heuristics outlined in \secref{sec:simulationheuristics}. Broader improvements may be achieved by finding ways to let it conduct more simulated games, either by making the Simulator faster, finding a more efficient combinatorial optimization algorithm or by offering users more input over how long they are content to wait for it. It is, in sum, a successful prototype for an ambitious deckbuilder support app. 
\documentclass{article}
\usepackage{graphicx}
\usepackage{array}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{gensymb}
\usepackage{amsmath}
\usepackage{array}      % for m{}/p{} column types
\usepackage{makecell}   % for better line breaks in cells
\usepackage{geometry}   % optional, for page width
\usepackage{listings}
\addbibresource{bibliography.bib} % your .bib file

\newcommand{\Land}[1]{\text{Land}(#1)}
\newcommand{\Spell}[1]{\text{Spell}(#1)}
\newcommand{\Plains}{\text{Basic Plains}}
\newcommand{\Island}{\text{Basic Island}}
\newcommand{\Forest}{\text{Basic Forest}}
\newcommand{\Mountain}{\text{Basic Mountain}}
\newcommand{\FloodedStrand}{\text{Flooded Strand}}



\title{LandFill}
\author{Leah Liddle}


\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
\subsection{Overview}
Magic: The Gathering (MTG) is a Trading Card Game (TCG) designed by Wizards of the Coast (WOTC). Players take the role of a wizard, whose deck is a library of spells with which they battle one or more similarly equipped opponents. Pursuit of the hobby thus involves mastery of both gameplay and deck construction. While tournament-level participation relies on ``netdecking'' - copying a decklist with a history of competitive success - the hobby is intended to have a significant creative component, with the unranked Commander format gaining popularity in recent years through its focus on unique and personal decks \cite{CommanderFormatOverview}.

In addition to spell cards, decks also contain ``land'' cards, which generate ``mana'', a resource expended to cast spells. A deck's lands collectively form its ``manabase''. Mana comes in five colours: White, Blue, Black, Red and Green (abbreviated respectively to W, U, B, R and G, or WUBRG collectively). Spells require specific colours, and lands produce one or more colours. A deck including spells of many colours has access to more spells, but runs an increased risk of a ``colour screw'', in which the player does not draw lands of the colours they need. Whereas selecting a list of spells is a stimulating creative pursuit, choosing a deck's manabase is less so: within a set budget, and notwithstanding the minority of lands which come with effects outside mana provision, there are objectively lists of lands that will maximize a player's chances of being able to play their spells. 

LandFill is a webapp that automates this aspect of deck building. Users input a list of spell cards, select some broad preferences for their manabase, and are provided with a manabase that has been optimized within those preferences to facilitate reliable casting of their spells. This optimization is necessarily approximate: Churchill \textit{et al} have demonstrated that, as it is possible to construct within an MTG game a Turing Machine whose halting is the necessary condition for a player's victory, a deck's winning strategy is undecidable \cite{churchill2019magic}, meaning that it is beyond LandFill's capacity to determine what lands support the most decisive plays. Nevertheless, the heuristic that the winning MTG player is typically the one who spends the most mana over the course of the game \cite{KarstenCurve} provides an opening for approximating the optimum via Monte Carlo methods. Using a stripped-down MTG simulator, in which the simulated player aims only to spend as much mana as possible each turn, LandFill estimates over iterated games how effective a given manabase is. It then produces successively optimized manabases via a Hill Climbing Algorithm. While LandFill may never rival the experienced eye of a seasoned competitive deckbuilder, it is nevertheless my contention that an app that provides a list of lands of demonstrated efficiency, via a click of a button, would pay dividends for casual players in ease of deckbuilding and satisfaction of games. 

\subsection{Thesis Layout}
My writeup here documents the development and testing of LandFill. I will begin by outlining relevant MTG rules and design trends, and the difficulties these introduce for optimization, followed by an overview of how LandFill will approach these. I will then outline my initial consultations with MTG players, and the features they require from a manabase optimization app. Dividing LandFill into four components - a MTG card database, an MTG game simulator, an optimization algorithm implementer, and a user interface - I will then outline its structure, and justify the decisions made during development. I will then summarize a second round of user-testing, and outline the value added by LandFill. I will conclude with an evaluation of the strengths and limitations of the app, and areas for future development.

\section{Problems and Proposed Solutions to MTG Manabase Optimization}
This section will provide an overview of MTG's rules and design practices insofar as they relate to manabase optimization, and the challenges which emerge form this. It then outlines the algorithmic responses to these challenges that will be used by LandFill. It will then touch on how incorporation of this into a deployable consumer product will be tested and developed. Finally, it will situate LandFill within existing scholarship on MTG automation and optimization.

\subsection{Gameplay Concepts and Terminology}
\subsubsection{Overview}
MTG can be played in several ``Formats'', with different deckbulding stipulations but largely identical rules, an overview of which is provided here. 

At the start of an MTG game, each player shuffles their deck (sometimes referred to as a ``library'') and draws a ``hand'' of seven cards. A player may ``mulligan'' a poor hand, shuffling it back into the deck and drawing a fresh one. While mulligan rules vary, players typically incur an increasing penalty for each mulligan performed. One additional card is drawn at the start of each turn. Cards in hand may be played onto the ``battlefield''. Each turn, a player may play one land card, which they may use once per turn by ``tapping'' it (turning it 90\degree). All lands untap at the start of each turn. A player, if they draw sufficient land cards, should therefore have access to one mana on their first turn, two mana on their second turn, and so forth. Spell cards which increase the amount of mana available per turn at a higher rate than this are called ``ramp'' spells. 

The ``mana cost'' of most spells includes a generic cost, payable by any mana, and any number of ``pips'', each representing a single required colour. Fig. \ref{fig:card1} shows a card requiring two black mana, one red mana, one blue mana, and four generic mana. It would thus be said to have a "cost" of UBBR4, and a "converted mana cost" (CMC) of eight. The spread of CMCs across a deck's spell cards is called its ``curve''.  

\begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/e01-85-nicol-bolas-planeswalker.jpg}
  \caption{A spell card}
  \label{fig:card1}
\end{figure}

Each colour is produced by a ``basic land'': Plains (W), Island (U), Swamp (B), Mountain (R) and Forest (G).  Whereas no deck may contain more than four copies of the same card (sometimes one copy, in ``singleton'' formats), any deck may contain any number of basic lands. In addition to the type "land", a land card may have subtypes, providing opportunities for synergy. Confusingly, the five basic lands, in addition to being named cards, are also card subtypes, collectively called the five ``basic landtypes''. Tropical Island, for example, is a non-basic land which has the subtypes Island and Forest. Any card, therefore, which references ``an island'' or ``a forest'' could have that criteria met by Tropical Island, a Basic Forest or a Basic Island. Within MTG player parlance, which will be used in this text, saying ``a Forest'' may refer to any card with the Forest subtype, while saying ``a Basic Forest'' refers to the specific card named Forest (Basic Forest, helpfully, also has the Forest subtype). 

Any card with a basic landtype taps for the corresponding colour of mana by default. However, not every card that taps for that colour of mana has that subtype. Tropical Island, for example, taps for both G and U, as does Hinterland Harbour, which is neither a Forest nor an Island. 

Although a small minority of lands produce more than one mana per tap, this is exceptionally rare. Through this writeup, a land which land which ``produces BUG'', taps for Black, Blue \textit{or} Green. Reflecting this, I will use the below standard to represent a player's hand or ``battlefield'' (zone to which cards are played from the hand):

\[
\text{Hand} =
  \begin{bmatrix}
    \Spell{RUG} & \Land{RU} & \Land{\Mountain} & \Land{\Island}
  \end{bmatrix}
\]

If a card is identified by a set of pips instead of a name, note that if it is a spell this refers to the colours it \textit{requires}, while if it is a land it refers to colours it \textit{can produce}. In the above hand, the Spell requires Red \textit{and} Blue \textit{and} Green mana, while the first land produces Red \textit{or} Green.

Lands that produce two colours are called Dual Lands, while lands that produce three are called Tri Lands; the small subsect of lands that can produce all five colours are called WUBRG lands. Some lands can produce colourless mana (C), which is only useful in generic costs. Since 2015, WOTC have printed some spells which require specifically colourless mana, but this is rare. 

Lands which provide an ability outside mana production are called ``utility lands,'' and are irrelevant to LandFill's calculations.

\subsubsection{Land Balancing and Cycles}
\label{sec:balancinglands}
Within WOTC design principles, ff card A is better than card B in at least one way, and worse in no ways, A is considered ``strictly better'' than B \cite{StrictlyBetter}. Tropical Island, tapping for UG, is strictly better than both Basic Island and Basic Forest. Since early sets, however, WOTC have generally avoided printing land cards which are strictly better than basic land cards \cite{GetReadyToDual}. Virtually all land cards which produce more than one colour of mana are either ``balanced'' (given a downside), or produce mana via a more complex mechanism. Breeding Pool, for example, is identical to Tropical Island save that it enters already tapped (and thus unusable on the turn it is played) unless the player pays 2 life when playing it. Lands are typically printed in cycles, which share a common balancing mechanism but produce different colours. Stomping Ground, for example, has the same stipulation as Breeding Pool, but produces RG instead of UG. Cycles usually carry informal names within the community: Breeding Pool and Stomping Ground are both ``Shock Lands,'' while Tropical Island is an ``Original Dual Land.'' The prevalence of cycles such as Check and Fetch lands (see Fig. \ref{fig:cycles}), additionally means that a cycle with basic landtypes may be considered strictly better than an identical cycle without. Lands which always enter tapped are called ``taplands''

\begin{figure}
    \centering
  \includegraphics[scale=0.4]{card_images/TwoCycles.jpg}
  \caption{UW and RG lands of different cycles. Left to right: ``Battle'', ``Shock'', ``Check'', ``Fetch'', ``Filter'' and ``Slow'' Lands. Notice the ``hybrid'' mana cost of the filter land's second ability, meaning that requires a mana of either of its colours to activate, and the diamond marker in the output of its first ability, meaning that said ability only produces colourless mana}
  \label{fig:cycles}
\end{figure}



To illustrate the complexity this introduces to optimization, consider as an example the lands Prairie Stream and Deserted Beach, both referenced in Fig. \ref{fig:cycles}. Since any situation in which Prairie Stream would enter untapped would also allow Deserted Beach to enter untapped, but the same is not true vice-versa, Deserted Beach is, taken in isolation, a stronger land. Consider, however, the following situation (in which all land cards are depicted in Fig \ref{fig:cycles}). Two players play two identical UW decks with identical UW manabases consting of Flooded Strand, Hallowed Fountain, and multiple Basic Plains and Basic Island cards. The only difference is that one includes a Prairie Stream, and the other includes a Deserted Beach.

Both decks draw the below opening hand:

\[
\text{Hand} =
  \begin{bmatrix}
    \Spell{UWW} & \Spell{UUW} & \Spell{UU} & \Land{\Plains} & \Land{\FloodedStrand}
  \end{bmatrix}
\]

Since the player needs copious amounts of both U and W mana, it behoves them to use the Flooded Strand to search their library for a non-basic Plains or Island capable of producing UW. Since this hand contains no spells of CMC=1, there is no disadvantage to playing a tapped land on their first turn. The Prairie Stream player may then go and fetch the Prairie Stream at no downside. However, the Deserted Beach player has to fetch the Hallowed Fountain, leaving Deserted Beach in their library. Consider, then, that when each player shuffles for the Flooded Strand's effect, the UW land remaining in their respective library (Deserted Beach for the Deserted Beach player, and Hallowed Fountain for the Prairie Stream player) is placed on top. Since the Hallowed Fountain can come in untapped for a trivial life point investment, the Prarie Stream player therefore has the option of playing the spell that costs UU, whereas the Deserted Beach player - able to play only a tapped Deserted Beach or a Basic Plains, which produces only W - cannot do so. 

Therefore, the appropriateness of playing a Prairie Stream vs a Deserted Beach in a given manabase depends on, in addition to other considerations such as the presence of Check Lands:
\begin{itemize}
\item The probability of a player beginning a turn with \(N\) lands on the battlefield in which \(N\) is greater than 1 and at least two lands are basic. 
\item The probability of a player beginning a turn with \(N\) lands on the battlefield, a fetch land in hand, and no possible set of spells to play with combined CMC of \(N+1\)
\end{itemize}

Optimization, therefore, may be thought of as a simultaneous two-part process:

\begin{itemize}
\item Replacing Basic Land cards with multicolour lands that improve \(P\), until a point is reached at which the accumulated downsides of those lands start to outweigh the diminishing returns from that improved access\dots
\item ...while choosing the multicolour lands whose downsides are the most significantly ameliorated by the specific spread of CMC and cost values of the spells in the deck, and by the interactions between those lands and other lands in the manabase. 
\end{itemize}

This makes manabase generation a Combinatorial Optimization problem. If a deck requires a manabase of \(M\) land cards, and operates with a performance of \(P\) for any given manabase (assigning a single performance metric for a deck is complicated; see \ref{sec:choiceofobjectivefunction}), the question is what combination of \(M\) cards from the set of all lands that produce one or more of the deck's colours maximizes the value of \(P\). 

There are therefore two problems to solve. The first is to find a method for determining \(P\) from a particular deck. The second is inherent to most Combinatorial Optimization problems: given that these typically deal with more candidate solutions than can be feasibly investigated seperately, it is necessary to find a method of optimization that will traverse only a relevant subsection of the total search space. 

\subsection{Simulation and Optimization}
LandFill approaches this problem via the implementation of two algorithms: an internal algorithm, which simulates MTG games and assesses the performance of a given manabase, and an external one, which provides the internal one with a series of increasingly optimized decks to test. These will be referred to as the ``Simulator'' and the ``Optimizer'', and the broad structure of both, and the relationship between them, will be introduced below.

\subsubsection{LandFill Simulation}
\label{sec:simulationoverview}
In MTG parlance, ``Goldfishing'' refers to testing out a deck by taking repeated turns against no opponent, and assessing the performance of the cards in the absence of opposing disruption \cite{GoldFishing}. Simulating all possible spell card interactions is an unfeasible undertaking here, and not necessarily a helpful one: Churchill \textit{et al} have demonstrated that, as it is possible to construct within an MTG game a Turing Machine whose halting is the necessary condition for a player's victory, even a deck able to model all spell interactions would not be able to interpret and execute any deck strategy\cite{churchill2019magic}. However, since our concern is only with the ability of a manabase to deploy spell cards, there is a useful heuristic to fall back on: in a game, the winning player is typically the one who spends the most mana \cite{KarstenCurve}. The simulator, therefore, need only try to spend as much mana as possible each turn. Broken down into constituent steps, the algorithm for a simulated game is as follows:

\begin{enumerate}
\item Draw an initial hand of 7 cards.
\item Mulligan as necessary.
\item Draw an additional card at the beginning of each turn.
\item Identify which playable land will allow the expenditure of M mana, where M is the maximum that may be spent that turn.
\item Play that land.
\item Play a combination of spells with total CMC \(C\). 
\item Repeat steps 3-6 for each turn of the simulated game.
\end{enumerate}

Within our heuristics, we may safely choose a combination of spells at cost \(C\) at random,  since we have no way of telling which would be relevant in any given game, and can only estimate their value as an efficient use of mana. However complexity is introduced by situations in which multiple lands allow for the spending of \(C\) mana, such as in the sample hand drawn in the previous section illustrating the appropriateness of Slow Lands vs Battle Lands: in the absence of any spells of CMC=1, playing either the Basic Plains or the Flooded Strand yelds \(C=0\). The simulator, therefore, must have some capability of assessing which lands maximize expenditure on future turns. 

\subsubsection{LandFill Optimization}
\label{sec:optimizationoverview}
In a Monte Carlo search, solution space is explored by taking the random outputs of stochastic processes, and sampling the resulting distribution to approximate the typical values of that process \cite{metropolis1949monte}. Assuming correct function of the aforementioned simulator, a Monte Carlo assessement of a manabase would be conducted by giving it a deck, running a large number of games with it, and recording the deck's average performance. The performance of a deck fed into a large number of games becomes \(P\) the objective function for the combinatorial optimization problem outlined at the end of \ref{sec:balancinglands}. 

LandFill's search space is \(L\)-dimensional, where \(L\) is the number of lands required by the deck, in which each dimension represents the quantity of a given land in the manabase (limited, in the Commander format, to values 0 or 1 for all non-basic lands). LandFill uses a variant of Hill Climbing optimization, also known as Neighbourhood Search. In Hill Climbing optimization, once an initial solution $\underline{x}_c$ is determined, all solutions in its neighbourhood $N(\underline{x}_c)$ - i.e.,~all solutions which differ from by some simple transformation $\underline{x}_c$ - are examined. The first solution to return a higher value is adopted as the new $\underline{x}_c$ \cite{silver2004overview}.  In the context of manabase optimization, the iterations are as follows:

\begin{enumerate}
\item Generate an initial manabase, $\underline{x}_c$, for the input deck. 
\item Conduct many simulations and determine the sample average performance, $F(\underline{x}_c)$ , of the deck. By the law of large numbers, this can be treated as an approximation for the true average performance.
\item Exchange a land in the deck for a different one, creating a new manabase, $\underline{x}_t$. 
\item If $F(\underline{x}_t)$ \(>\) $F(\underline{x}_c)$, adopt $\underline{x}_t$ as the new value of $\underline{x}_c$, and return to step 2. 
\item If not, return the original land to the manabase and return to step 2, this time making a different, previously unexplored substitution. 
\item If all lands in the deck have been systematically replaced with every candidate land that could replace them, and no value has exceeded $F(\underline{x}_c$), return $\underline{x}_c$ as an optimized manabase.
\end{enumerate}

Since Hill Climbing is a ``greedy'' optimization algorithm, it searches for \textit{local} rather than \textit{global} maxima. There exist viable optimization algorithms, such as Simulated Annealing, which are less susceptible to this\cite{silver2004overview}. However, given that the simulator itself can only loosely approximate the decision-making process of an actual MTG Game, LandFill falls into the category of optimization problem for which ``the optimal solution'', as described by Zanakis and Evans, ``[is] only academic'' \cite{zanakis1981heuristic}. To be a valuable product, LandFill needs only to be able to create a more reliable manabase than a human player could in a comparable length of time.

\subsection{Development and Testing Methodologies}
In software development, ``Verification'' testing tests code functionality according to designer specifications, while ``Validation'' testing tests whether the code meets user needs \cite{verificationandvalidation}.  The schedule for each of these is detailed below

\subsubsection{Verification Testing}
My schedule for verification testing was couched within an ``Iterative'' development process, whereby each feature was independently designed and tested before being added to the core product\cite{iterativedevelopment}. The Simulator and Optimizer are two of four component subsystems to LandFill, arranged like so:

\begin{enumerate}
\item The \textit{Database} - stores information about MTG cards. 
\item The \textit{Simulator} - simulates games using information from the database.
\item The \textit{Optimizer} - assessess decks using performance data from the simulator. 
\item The \textit{Interface} - allows for use of the optimizer by a lay customer.
\end{enumerate}

Since each component utilizes the one before it,, LandFill naturally lends itself to an Iterative Development process. The classic alternative of ``Waterfall Development'', in which all testing is withheld until product completion \cite{iterativedevelopment}, would in this case forbid me from factoring in shortcomings of a given component into the design of the component superceding it if those shortcomings were not discovered until after development. Moreover, iterative development puts LandFill in good stead for its anticipated lifecycle. Since WOTC regularly print new cards, and each new mechanically distinct land must be individually coded, LandFill will always need to be able to accept new additions to its codebase. 

Since verification testing is to be conducted on each component seperately, testing approaches for each component will be outlined in the section of this thesis that deal with that component.

\subsubsection{Validation Testing}
Since LandFill's viability ultimately rests on its comparison to a human deckbuilder and not in its ability to find a consistent global optimum, it must:

\begin{itemize}
\item Be flexible enough to fit easily into a range of different deck construction strategies.
\item Accommodate, via user input, game-extrinsic manabase restrictions such as price, availabilty and personal preference. 
\end{itemize}

To ensure this, in addition to validation testing conducted after development of the initial product, I conducted user research prior to development, to ensure that user needs were accounted for throughout the design process. User research consisted of the following:

\begin{itemize}
\item A series of Semi Structured Interviews, which gauge the way prospective users create MTG decks and the manabases thereof. 
\item A series of Think-Aloud evaluations of a mockup, in order to observe the patterns a user falls into when using an app to this purpose.
\item A Kano Questionnaire, the questions of which are based on results of the previous evaluations, in order to guide development priorities. 
\end{itemize}

Validation tests, conducted after the development, consisted of the following steps.

\begin{itemize}
\item Two seperate deckbuilding exercises, one of which allowed use of LandFill, and one of which did not.
\item Two Task-Load Index (TLX) questionnaires, to determine the relative difficulty of each. 
\item A semi-structured interview to assess the testee's opinion of the software, and their suggestions for future developments.
\end{itemize}

\subsection{Relevance to Existing Material}
\label{sec:relevancetoexistingscholarship}
LandFill engages with three areas of prior research:

\begin{itemize}
\item Academic interest in MTG automation. 
\item Use of computer models in manabase analysis within the MTG player community.
\item Existing deckbuilding apps.
\end{itemize}

I will outline its engagement with these areas below.

\subsubsection{Academic Interest in MTG Automation}
Much modern academic interest in MTG, from a computer science perspective, rests on Ward and Cowling’s landmark 2009 paper, “Monte Carlo Search Applied to Card Selection in Magic: The Gathering”. Ward and Cowling hypothesize a that methods used to automate other imperfect information games, such as Bridge and Poker, may be applied to MTG \cite{ward2009monte}\cite{esche2018mathematical}\cite{alvin2021toward}. A decade hence, interest in this problem has resurged thanks to the rollout of WOTC’s virtual MTG venue, MTG:Arena, which, although primarily a player-vs-player (PvP) engine, features an AI opponent, nicknamed Sparky, who, although useful in gameplay tutorials, presents no challenge to experienced players\cite{alvin2021toward}. 

Ward and Cowling point to the inherent difficulty of automating games of imperfect information: strategic thinking in MTG is confounded by the unknowability of both the opponent's hand and the next card to draw. Since LandFill's goal is simply to spend as much mana as possible each turn, these concerns are both rendered irrelevant: the hand is simply a set of resources to be allocated as efficiently as possible. This puts it somewhat outside the realm of scholarship inaugurated by Ward and Cowling, which is chiefely concerned with how to encode strategy given the complexity of gameplay and the limited information. Drawing on methods used in other imperfect information games, such as Go and Poker, Ward and Cowling's automated players use Monte Carlo analysis to examine possible outcomes of combat between creature spells once cast \cite{ward2009monte}, while Alvin \textit{et al} explore graphical representation of card synergies \cite{alvin2021toward}; in both cases, effective use of lands to play spells is trivial, as the deciding factor in which spell to cast is based on the exigencies of the game state. Indeed, Esche's 2018 research into optimal MTG strategy eschews casting any multicolour spells, testing his virtual player only on a mono-red deck \cite{esche2018mathematical}. LandFill's simulator represents a small engagement with this line of research, as it offers a fast method of determining optimal mana usage.

Automation of deckbuilding, rather than of play, is more relevant to LandFill but less well studied. Sverre Johann Bj{\o}rke and Knut Aron Fludel explore the use of a genetic algorithm to generate decks out of a set card pool (as is done by human players in the ``Limited'' format); however, their results are inconclusive. As their work relied on a full automated AI player - which, as established, still underperform compared to human players - their generated decks only performed well against other decks also played by the same automated player, and faltered against human opponents. LandFill represents a narrowing of the ambitions of Bj{\o}rke and Fludel's work. From a purer mathematics perspective, Riccardo Fazio and Salvatore Iacono have conducted some research into how to quantify the mana requirements of a deck; however, their work is limited only to assessing the quantity of each colour required, and not how these quantities are spread across mechanically distinct lands.

\subsubsection{Manabase Analysis within the Player Community}
Guides exist on how to write basic scripts in order to use Monte Carlo techniques to analyse a given deck \cite{MonteCarloGuide}. A central figure in this practice is Frank Karsten, who has written extensively on how to determine a manabase for a multicoloured deck. His seminal series of articles, \textit{How Many Sources Do You Need to Consistently Cast Your Spells?}, use Monte Carlo search to produce a grid (see Fig. \ref{fig:KarstenCurve}) outlining how many lands of colour \(C\) you need depending on the most demanding spell requiring colour \(C\) in your deck. Rather than simulate gameplay, Karsten draws random hands for each possible spell (abstracted to just its mana cost), to determine the number of lands of the spell's colour that would need to be included in the deck in order to reliably cast it on the turn when the \(M'th\) land is played, where \(M\) is the spell's CMC.


\begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/KarstenCurve.jpg}
  \caption{Frank Karsten's recommendations for how many lands a deck should include capable of producing colour C; he recommends adopting the highest Y-axis score corresponding to a card whose mana cost appears in your deck \cite{KarstenCurve}}
  \label{fig:KarstenCurve}
\end{figure}


While this article engages broadly with the issue of how many multicolour lands are necessary in a deck before they begin to incur diminishing returns, it cannot assess when these diminishing returns become a greater handicap than expanded colour access. To understand whether that land will be able to use its full productive capacities when played as the \(M\)'th land requires an understanding not just of the coloured lands drawn previously, but the behaviour of these lands in a state of play. This is where I believe the implementation of a stripped down play simulator will pay dividends. 

This approach has been used by Karsten elsewhere, to analyse manabase choices in the context of the Limited format (which features small decks and a heavily restricted set of lands to choose from)\cite{TappedDualsLimited} and the Standard format around the release of the Ixalan Set\cite{IxalanManabases}. In the latter article, Karsten's engagement is limited to one nonbasic land cycle - Check Lands - and only to their probability of entering tapped, not the probability that their entering tapped incurs a gameplay downside in the context of a given deck. In the former article, Karsten engages explicitly with the question of at what point the disadvantages of dual lands outweigh the advantages of greater colour access, making this work a natural precursor to LandFill. Karsten's simulator is likely to converge on a more reliable performance estimate for a given manabase, running orders of magnitude more times than LandFill does. However, in addition to lacking deck construction functionality, its simplified game simulation model only uses one type of nonbasic land, and utilizes a much more straightforward decision making process. Ultimately, this restricts this analysis into simply a matter of deck-building best-practices. LandFill, therefore, may be seen as an attempt to adapt Karsten's methodology for this area of his research into into a widely applicable deckbuilding tool in the vein of his grid. 

\subsubsection{Existing Deckbuilding Apps}
I have identified three pieces of software which support the same phase of deck-building as LandFill, and will examine their functionality below. 

The first is MTG: Arena \cite{mtgarena} (screenshot in Fig. \ref{fig:arenascreenshot}), which is capable of automatically filling a deck with an appropriate proportion of each basic land in accordance with the deck's colours. While this is relevant for the Standard and Limited formats largely played on Arena, the deckbuilding restrictions of which only offer few nonbasic lands, it is less appropriate for other formats, in which a sizeable proportion of a manabase will be nonbasic; it is also only accessable to decks constructable via the limited set of cards available on MTG Arena. 

\begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/MTGArenaScreenshot.jpg}
  \caption{A screenshot of a deck under partial construction in Arena, with the decklist on the right-hand side column. Basic Plains and Basic Islands are automatically added as white and blue spells are.}
  \label{fig:arenascreenshot}
\end{figure}

The second is the website ManaGathering \cite{managathering} (screenshot in Fig. \ref{fig:managathering}), a database of nonbasic lands sorted by colour. Players input the colours of their deck and are given a list of nonbasics within those colours, sorted by cycle. Although ManaGathering has no optimization or manabase generation facility, it fulfills a similar role in the deck building process as LandFill, facilitating the choice of generically strong mana-producing lands once utility lands or lands fulfilling a niche strategic concern of the deck have been chosen. A broad success metric for LandFill is that it should return a better result than a player using ManaGathering could in a comparable time.

\begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/ManaGathering.jpg}
  \caption{A zoomed-out screenshot of an excerpt from the ManaGathering page for a WUB deck. Note the WU, UB and BW lands arranged by cycle.}
  \label{fig:managathering}
\end{figure}

Finally, the website Archidekt \cite{archidekt} (screenshot in Fig. \ref{fig:archidekt}) combines a basic-land allocator \textit{a la} MTG: Arena with a communal "package" system. Players create "packages" of lands which are saved publically on the site, and may be imported by other players. For example, a player may get a list of colour-appropriate lands of a reasonable price by importing a given manabase package and then automatically generating a list of basic lands. The question of whether LandFill or ArchiDekt are capable of producing more reliable manabases is unlikely to be answered in the timeframe of this investigation, but LandFill represents an alternative approach; morever, as Archidekt's functionality is limited to decks that are stored within its database, it offers a more flexible service to deckbuilders who may prefer other collection-tracking databases such as TappedOut or Moxfield.

\begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/ArchidektScreenshot.jpg}
  \caption{A screenshot of community created land packages on Archidekt, categorized by, among other things, price and color. These can be imported into a decklist.}
  \label{fig:archidekt}
\end{figure}

It is worth noting that while Archidekt is the only card database to offer this feature, it is not the only card database. Others include Moxfield \cite{moxfield}, TappedOut \cite{tappedout} and Deckbox \cite{deckbox}, all of which allow the player to upload a decklist to be stored and visually displayed. Although providing very different value to LandFill, they will be referenced throughout this writeup, as, as will be covered in \ref{sec:initialusertests}, decklists both entered into and extracted from LandFill will often likely be moving to and from such databases.


\subsection{Library/Language Choices}
My choices of language and libraries were informed by two main priorities. Due to my short turnaround time, it was important that I use libraries and langauges with substantial community support for web development. Meanwhile, as a usable app, LandFill benefits from high performance so as to maximize the number of simulations it can run, but does not need to offer a complex user interface nor store user data, prompting me to favour high-performance tools over complex and scalable ones.

In places where these requirements are at odds, I prioritized the former: my choice of Python as a backend and Javascript as a frontend was driven largely by the popularity of these languages in web design. However, in other decisions, the two requirements informed each other constructively. I chose Flask \cite{flask} as a backend web framework as its simplicity made it both easy to learn and reputably faster; contenders like Django \cite{django} are made both slower and more complex due to their abundance of features (FastAPI \cite{fastapi}, potentially lighter and faster than Flask, was discarded due to its smaller userbase and thus relative paucity of learning resources). React \cite{react}, which I chose as my frontend framework, similarly sports a wealth of support resources, and features a Virtual DOM that lowers performance overheads when users make small input changes - a relevant concept here, as users will likely order several simulations with small preferential changes on each one. 

The use of Object Relational Mappers (ORMs) is common in web design, usually as a component of a CRUD (Create, Read, Delete, Update) interface taking user data. Although LandFill makes heavy use of Object Relational Mapping to store a database of MTG cards, the user needs to only read the database. For this reason, SQLAlchemy \cite{sqlalchemy}, an ORM esteemed for rapid performance at the cost of easy data amendment \cite{WhyNottoUseSqlalchemy}, was the obvious choice. 

During development, I sometimes used the Large Language Model ChatGPT \cite{chatGPT}. The majority of my usage was to help identify bugs in my code. I also used it to suggest libraries or standardized testing methodologies (i.e.,~the Kano Questionnaire) after providing an outline of my use case; in all these cases, I researched its suggestions thoroughly after recommendation. Later in development, I commissioned it to write sections of code for me based on a pseudocode outline. 

\subsection{LandFill Structure}
With both the inherent challenges of manabase optimization and the set of potential user needs thus established, LandFill will be built as four interacting components, outlined below.

\begin{itemize}
\item The Database - LandFill's database of MTG card objects, stored in the backend as mtg.db.
\item The Simulator - see \ref{sec:simulationoverview}
\item The Optimizer - see \ref{sec:optimizationoverview}
\item The Web Interface - a user-friendly web interface designed according to the priorities set out in \ref{sec:initialusertests}
\end{itemize}

\section{Pre-Development User Research}
\label{sec:initialusertests}
\subsection{Overview}
I began development by holding one-on-one user research sessions with eight MTG players. The sessions were divided into two parts, a Semi-Structured Interview and a Think-Aloud Mockup Test, with an additional Kano Questionnaire circulated to users afterwards. 

\subsubsection{Semi-Structured Interview}
In addition to specific feedback on possible features, I was interested in understanding more about how players typically create manabases. I therefore chose to lead with a Semi-Structured Interview. This is a data-gathering method in which the interviewer stays flexible on how and in what sequence questions are asked, to allow unexpected themes and topics to emerge \cite{manuel2004sage}. My questions were as follows:

\begin{itemize}
\item How do you approach selecting lands for a deck, and how does this vary across formats you play?
\item How do you approach acquiring lands for a deck (eg, do you assemble a list of cards to purchase, do you assemble a list of cards you already have - and if so, do you have a good knowledge of what lands you own)?
\item What role do existing deckbuilding support apps, such as Moxfield and TappedOut, play in your process?
\item Do you factor the strategy of your deck into land choices in terms of pure mana production (i.e.,~not including utility lands).
\item How do you mulligan? How does this vary across formats that  you play?  
\end{itemize}

\subsubsection{Think-Aloud Mockup Testing}

In a ``Think-Aloud Evaluation'', users are asked to narrate aloud their thoughts and opinions while attempting to use a system \cite{wright1991use}. This is an appropriate method for early development since it can be conducted on a ``mockup'', an aesthetically versimillitudinous but non-functional representation of the planned interface. Users are occasionally prompted for input, and may be given solutions to problems if necessary, but are largely expected to use the product unassisted. The LandFill mockup presented to users is displayed in Fig. \ref{fig:mockup}. 

\begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/MockupFrontend.jpg}
  \caption{Draft front-end used in the mockup testing.}
  \label{fig:mockup}
\end{figure}



\subsubsection{Kano Questionnaire}
\label{sec:kanoquestionnaire}
Kano Analysis is an approach to user evaluation that examines the emotional response of a prospective user to the presence or absence of a given feature. I developed a Kano Questionnaire after analysis of the Interview and Think-Aloud data, so as to prioritize which features, suggested by individual testers, were reflective of more widespread demand. A generic Kano template is displayed in Fig. \ref{fig:kanoq}. My questionnaire listed the following proposed features:

\begin{itemize}
  \item The option to exclude from consideration all lands which always enter tapped.
  \item The option to exclude from consideration all lands above a certain price. 
  \item The option to exclude any individual land or cycle from consideration via the player's own preference. 
  \item The option to mark some lands as mandatory for LandFill to include.
  \item The option to ``weight'' lands, so that LandFill prioritizes a player's preferred cycles in its evaluation.
  \item The option to input a list of lands as well as a list of nonlands and have LandFill choose the best of these, rather than explore all possible lands.
  \item The option to tell LandFill not to recommend ``Off-colour Fetches''. For example, a Fetch Land that can search for an Island or Plains can still fetch Islands in a UR deck. This was considered distasteful by one test subject.
  \item The abiilty to see an image and description of any suggested land/cycle.
  \item The ability to view performance metrics for the deck after manabase generation.
  \item A FAQ explaining how LandFill determines lands.
  \item The ability to specify how much life a player is comfortable to lose to land cycles that require a life point investment, i.e.,~Shock Lands.
  \item The ability to view low-performing lands in a given deckilst, to inform a player's choice of supplemental mana generation spells, or what to replace on the release of new cycles. 
  \item The ability to generate manabases for, respectively, the Commander, Modern, Legacy, Pauper and Limited formats.
  \item The ability to copy a decklist into and out of LandFill with minimal reformatting from, respectively, the following database apps: TappedOut, Archidekt, Moxfield, Deckbox. 
\end{itemize}

\begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/KanoSurvey.jpg}
  \caption{A question in a generic Kano questionnaire}
  \label{fig:kanoq}
\end{figure}

\subsection{Emerging Themes}
While initial user-testing yielded multiple small design considerations, and will be cited as various development decisions are outlined throughout this writeup, key themes are outlined below.

\subsubsection{Commander/Casual Preference}
Commander, Limited and Pauper were the three most popular formats among interviewees. Of these three, a plurality of interviewees acknowledged that Commander would be the only format for which they would consider using LandFill. The other two were disqualified by users in general on the following grounds:

\begin{itemize}
\item Limited - in this format, cards are assigned to a player at random immediately before a game. Decklists are therefore not generally uploaded to any databases, and inputting all cards into LandFill, especially on a mobile device, would be impractical.
\item Pauper - this format restricts decks to only common and cheap cards. In Pauper, use of utility lands, or lands with specific synergistic qualities, is so normalized that manabase strategies prioritize this over consistent mana generation.
\end{itemize}

While fewer interviewees had much experience with more competitive formats such as Standard, Legacy and Modern, several interviewees raised doubts as to the ability of any spontaneous manabase generator to gain traction within those scenes, as players in those formats habitually use existing deck archetypes, and attach to them manabases with proven competitive credentials. In responses to the Kano Questionnaire, most respondents expressed that they would like/be neutral on support for non-Commander formats, but only one respondent identified any format other than Commander as a minimum expectation. 

I therefore decided that I would restrict LandFill to being solely a Commander product for initial development. Commander's deckbuilding restrictions are markedly different from other formats, and will be discussed at length in my outline of the Simulator (see \ref{sec:thecommanderformat}). I therefore deemed it more important to make a product that worked seamlessly for Commander decks, rather than one that attempted to accommodate potential decks across a wider range of formats. 

\subsubsection{Lack of Strategic Thinking but Strong Preferences}
\label{sec:nostrategy}
When asked about how the strategy of a given deck informed their choice of lands, nearly all test subjects said that it did so, but almost solely in regards to the ancillary effects of certain cycles – i.e.,~Gain Lands (tapped dual lands that increase the player's life point total by 1 on entry) being popular in decks that trigger from gaining life. Only one said that they would actively choose slower but more colour-diverse lands for slower decks. 

However, when presented with the Mockup outlined in \ref{fig:mockup}, all users swiftly devoted themselves to using the tickboxes to remove lands or cycles that they did not want to have included in the deck, despite having been told to imagine this mockup returning a fully optimized list of lands. Several users also expressed a desire for a weighting functionality, so that they may specify lands which they prefer. 

Some testees also expressed a dislike of certain categories of lands or cycles, i.e.,~any cycle of dual land that only ever enters the battlefield tapped, or any off-colour Fetch Land (see the relevant listed feature in \ref{sec:kanoquestionnaire}).

The lack of strategic decision-making in manabase assembly validates LandFill's use-case, as it suggests that, by testing lands against the needs of the deck, LandFill is putting more consideration into land selection than a typical player. However, it is clear that player input needs to include both:

\begin{itemize}
\item A positive component, in which players can force inclusion of potentially suboptimal lands such as Gain Lands due to deck strategy.
\item A negative component, in which players are given extensive leeway to remove lands from consideration. 
\end{itemize}

\subsubsection{Deckbuilder Personas}
\label{sec:personas}
Testees broadly designed manabases in one of two ways, embodied in the below personas:

\begin{itemize}
\item Persona A, who possesses a large collection of land cards and, on creation of a new deck, selects lands in the approrpiate colours from this collection.
\item Persona B, who develops a new deck and chooses lands based on abstract preference, and then orders those lands.
\end{itemize}

This prompted me to ask testees which of the following two models of LandFill they would prefer:

\begin{itemize}
\item Model 1 - LandFill, in addition to taking a decklist of nonland cards, also takes a list of land cards that the player might have found in their collection, and returns the optimum subsection of these.
\item Model 2 - LandFill takes a decklist of nonland cards and asks the user only to specify what lands they do not want, generating an optimized list from the remaining options.
\end{itemize}

I expected a preference towards Model 1 to be strongly associated with Persona A deckbuilders and vice versa, but to my surprise, a majority of deckbuilders across the personae preferred Model 2. Several Persona A deckbuiders preferred it because they were enthused by the prospect of being recommended lands they had not heard of before. 

While this was a majority consensus, and all Kano respondents said they would be able tolerate the absence of Model 1 functionality, some users did express a desire for Model 1 functionality, making it a viable route for future development of LandFill. However, the initial design covered in this writeup will adhere to Model 2. 

\subsubsection{Flexible Input Parsing}
All users polled made use of at least one online database (a plurality used TappedOut, with Moxfield and Deckbox also being popular). In the Think-Aloud evaluation, they copy pasted decklists from these sources into LandFill, and said they would return the list of outputted lands there. Interestingly, many copied not from the inbuilt export feature of these sites, but instead by simply copying the decklist as it is displayed on the page.

This suggests LandFill would benefit from a flexible input parsing device. LandFill should be able to accept a decklist pasted in from both the export features and front pages of all these databases, and format its outputs so as to be input into these services. Moreover, TappedOut and Archidekt allows for categorization of cards into custom types, which are included in the decklist when copied. The parser, therefore, must be able to recognise these custom types and parse them not as cards but as keys to a dict object which contains the list of cards corresponding to that category, and then display accordingly on output. This avoids players losing their custom categories when they utilize LandFill.

\subsubsection{Mulligans}
\label{sec:mulliganheuristic}
No testee was able to describe any consistent principles on which they based their decision to mulligan. All testees said the decision would be based not just on the castability of spells in the hand, but also the strategy enactable via those cards. It is not, therefore, my priority to provide a means to replicate a given LandFill user's mulligan preferences. For the initial design, the mulligan heuristic will be built into the simulator. 

\subsubsection{Life Loss, Cost, and the Knapsack Problem}
\label{sec:knapsack}
Because user testing was conducted before the development of the Optimizer, I suggested two proposed features which provied unimplementable:

\begin{itemize}
\item Total manabase cost.
\item The maximum amount of life a player was happy to lose to lands, such as Pain Lands and Shock Lands, which require a life point investment (this value is set via the "Pain Threshold" input on \ref{fig:mockup}; that testees unanimously found this wording confusing is ultimately irrelevant, as the feature was not included). 
\end{itemize}

These are theoretically easy to determine for any decklist. Manabase cost can be determined from the sum price of each card, data which can be easily included in the Database. While tracking life expenditure would be more difficult, as it would require the Simulator to try and minimize life point expenditure in a situation where there are multiple ways to spend the same amount of mana, it would also be possible to return the average life point penalty incurred over a Monte Carlo search along with the general performance metric. However, these are both examples of the 0-1 Knapsack Problem, itself a NP-hard combinatorial optimization problem, in which a subsection of weighted items must be chosen from a larger set that minimize the total weight (with weight, in this case, representing respective average life damage and card cost)\cite{martello1990knapsack}. Since this would involve simultaneously optimizing multiple areas of deck design, it would represent a sizeable increase in computational complexity for LandFill.

Surprisingly, subjects were broadly ambivalent on both of these features. During the Think Aloud evaluation, no subjects attempted to make use of either. Kano data introduces some more nuance. The ability to specify an upper limit of life loss was something all but one respondant would consider desirable, although none considered it a minimum requirement, nor its absence to be something they would "dislike". The ability to specify price limits performed similarly, save that one respondent did indeed consider it a minimum requirement. This suggests that, while it is not worthwhile to simultaneously attempt to optimize these metrics while optimizing mana production, these features are not useless, and it is worth finding some approximate representation of them in design. My approach to the issue of life loss is covered in \ref{sec:landprioritization}. My approach to price consideration is covered in \ref{sec:preferences}.

\section{The Database}
\subsection{Database Requirements}
The database in which LandFill stores card objects will need regular updating by the site owner. This is for two reasons:

\begin{itemize}
\item WOTC regularly releaseses new sets of MTG cards.
\item Since LandFill will factor price consideration into its choices, the database must reflect the up-to-date price of a given card.
\end{itemize}

In a given session, the database will be queried at two points:

\begin{itemize}
\item When a user's decklist is added, to determine the mana costs of nonland cards.
\item To identify a list of candidate lands for the manabase.
\end{itemize}

This creates two constraints for the database: it must be up-to-date, and have reasonable response times over multiple queries for individual objects. An early prototype used the Scrython API (see \ref{sec:databasesource}) to fetch requested cards from an existing online MTG database. While this guaranteed up-to-date information, it more than ten minutes to retrieve a list of 50 nonland cards due to a combination of connection latency and the need to avoid overwhelming the server. 

To accommodate both constraints, the database must store information locally, but be easily updatable. 

\subsection{Choice of Online Database - ScryFall/Scrython}
\label{sec:databasesource}
One popular source of MTG card data for developers is mtg.json, a json reprsentation of all cards. Mtg.json is popular for the breadth of information it contains, including card reprints, making it useful for development of shopfronts or collection management software \cite{mtgjson}. Since LandFill needs to know only the gameplay-relevant attributes of a card and its price, I favoured a more streamlined source. I ultimately chose to use the database Scryfall \cite{scryfall}, which is predomenantly used by players rather than programmers; I felt that this wider userbase would ensure up-to-date information. ScryFall offers an API for code integration, for which a python library, Scrython \cite{scrython} has already been developed. Scrython accepts card queries using the format of ScryFall's advanced search option, and returns a list of dict objects listing card attributes, referred to within LandFill's code and this writeup as ``SCOs'' (Scrython Card Objects). LandFill's database, thererefore, is a translation of these SCOs to SQLAlchemy's ORM format. Allowing for small pauses to prevent server overuse, downloading all ~30,000 cards (at time of writing) takes around one hour, and could easily be run weekly, or in response to new sets.

In addition to ease of access ScryFall has the advantage of listing the colours produced by a given land card, eliminating the need to parse this from the card text. Its most significant shortcomings pertain to price data. First, it only offers a price in EUR and USD and not GBP, and it fails to list the price for a small minority of cards. As a simple fix for this, LandFill uses the CurrencyConverter library to determine the price in GBP from the price in EUR. It also fails to list the price for some cards. A future version of LandFill could potentially scrape an alternate database on download if a price is not found on ScryFall, but as this only impacts 10 land cards that have, at time of writing, been encoded into LandFill, for intial development I have simply entered the prices manually.


\subsection{Database Layout}
Since LandFill uses Flask, it is built not on SQLAlchemy but on the slightly more feature-heavy Flask-SQLAlchemy extension \cite{flasksqlalchemy}. This means that each mapped class extends Flask-SQLAlchemy's \texttt{model} ancestor class, rather than the \texttt{declarative-base} class used in classic SQLAlchemy. The relational mapping between models in mtg.db is shown in Fig. \ref{fig:databaselayout}.

\begin{figure}
    \centering
  \includegraphics[scale=0.5]{card_images/mtg2.jpg}
  \caption{Layout of mtg.db}
  \label{fig:databaselayout}
\end{figure}

The two upper rows of tables in Fig. \ref{fig:databaselayout} map the many-to-many relationships which denote the availability of a card across formats and ``games'' (eg, physical MTG games, MTG: Arena); one card is legal in many formats, and one format has many legal cards. Since the initial deployment of LandFill focusses on the Commander format, this is largely irrelevant, but remains in the schema for use in future expansion. Discussion henceforth will focus solely on the models Card (table: cards), Cycle (table: cycles) and Face (table: faces). 

Although not every attribute of a SCO is embodied in a model, in this initial development stage, LandFill errs in favour of storing too much data rather than too little, meaning that some card attributes, including \texttt{Card.\allowbreak\_silver\allowbreak\_bordered} and \texttt{Card.\allowbreak\_last\allowbreak\_printing}, are not relevant to this writeup. I will define attributes throughout this writeup as they become relevant. 

\subsection{Database Management}
I developed a class, DatabaseManager, which contains simple methods that bulk-transfer card information from Scryfall to the Database via Scrython. Within my codebase, a single instance of DatabaseManager is created in one script, \texttt{manage\allowbreak\_database.py}. Simply running this script will fully update the database with all existing cards. Pauses are built into the script using Python's \texttt{time} module to avoid overburdening ScryFall's servers. 

To facilitate debugging during development, ScryFall is never queried for \texttt{all} cards. Instead, a for-loop is used to systematically acquire cards in order of mana cost, and add them to the database only after all have been downloaded. This means that, when errors occurred during download, I can resume the download from midway through the loop rather than starting from the beginning.

\subsection{Representing Multiple-Faced Cards}
Some cards have two faces, each of which may be considered two different cards with their own text and mana cost. Although most cards have only one face, I sought to keep the database in 1st Normal Form, a database normalization standard which stipulates each column in a databae contains only a single value per entry \cite{databasenormalization}. I therefore treat Faces as having a many-to-one relationship with cards. Broadly, the Face object has attributes used to determine its interaction with a given game state: i.e.,~its casting cost and potential basic landtypes (stored in the \texttt{faces.\allowbreak\_typeline} attribute), while the Card object holds attributes relating to the viability of a card within a deck and a given user's preferences (IE, what colours of mana it produces, the cost of the card).

The rules on how to play a card's different face varies between cards: some maybe played with either face, while others have a single playable face and swap to the other under certain game conditions. This is determined by the \texttt{cards.\allowbreak\_layout} attribute. A card's layout is one of several strings extracted from the SCO, each of which denotes a different way of formatting face relationships (i.e.,~cards with the ``transform'' layout have one playable face; cards with the ``adventure'' and ``mdfc'' layout have two). The playabiltiy of a given face is stored in the \texttt{faces.\allowbreak\_playable attribute.}

Some cards are a spell card on one side and a land card on another. A card is considered a land if it has at least one playable face that is a land, represented by the \texttt{card.\allowbreak\_overall\allowbreak\_land} attribute. 

\subsection{Cycles}
Recall from Fig. \ref{fig:cycles} land cards within the same cycle are mechanically identical but refer to different colours in their text. A card's memebership in a given cycle is, unfortunately, not included in a SCO. For this reason, each cycle has a string attribute, \texttt{cycle.\allowbreak\_regex}, consisting of a Regular Expression. If DatabaseManager matches the regex of a cycle to a card, that card is connected to that cycle via Foreign Key. Since some cycles are mechanically identical to each other but are distinguished by the presence of basic land types or whether the constituent cards have the ``snow'' card type, these are both also stored in the cycle object as Boolean values, to sort cards whose text matches multiple cycles.

Although some spell cards are arranged into cycles - mechanically similar cards where each costs a different colour or set thereof - this is irrelevant to LandFill, and the cycles table therefore stores only land cycles. 

\subsection{Database Verification Testing}
A secondary script, \texttt{test\allowbreak\_db.py}, conducts both Unit Tests and Property Tests on the database. 

In Unit Testing, the output of code is compared to a pre-computed result \cite{PropertyTesting}. In this case, Unit Tests check that there are an expected number of total cards and an expected number of total lands in each cycle. Unit testing cycles is important in ongoing maintenance, to ensure that any new regex patterns added for new cycles does not accidentally capture existing ones. 

In Property Testing, random inputs are generated for code, and outputs are checked to ensure they fall within broad parameters \cite{PropertyTesting}. In this case, the Property tests randomly selects a sample of database entries and checks that none have more than two faces, and that none have two faces with the same text. This is more expedient than testing all cards, and allows reasonable confidence that the database is populated as expected.



\section{The Simulator}
\subsection{The Commander Format}
\label{sec:thecommanderformat}
In the Commander format, deckbuilders choose a creature to be the ``Commander'' of the deck. Rather than being included in the deck, the Commander is placed in the ``Command Zone'', allowing them to be reliably cast in every game. Some cards are marked to allow a ``Partner'', a secondary commander also placed in the command zone. Because of this, Commander decks generally include cards that work synergystically with the commander in gameplay. 

Analysis by the Command Zone podcast suggests that the average Commander game runs for 10 turns per player \cite{CommanderMythBusters}. Unlike other formats, Commander is typically played with more than two players per game.

Commander games utilize the ``London'' mulligan rule, with a single ``free'' mulligan. This means that at the beginning of a game, a player must put \(N\) cards on the bottom of their library, where \(N\) is the number of mulligans they have taken after the initial free one \cite{commandermulligans}. 

\subsection{Classes}
\subsubsection{GameCard and subclasses} 
\label{sec:gamecards}
During simulation, a land must exhibit the unique behaviour of its cycle. LandFill, in order to execute the method overriding required here, requires a class hierarchy beginning with a general Card object, of which Spells and Lands are behaviourally distinct child classes, and each cycle represents a further child class of Land. 

SQLAlchemy does support Single Table Inheritance, in which subclasses with distinct methods are stored in the ancestor class's shared table; it is also possible to temporarily store game state information via cached properties without updating the database. However, in practice, it proved simpler to create a new abstract class, GameCard, for reprsentation of the Card object in game. The initiation method of each GameCard takes a Card ORM object as an argument, and extracts relevant data from it. This removes any risk of persisting data in between games. It also allows me to create SubLands, a descendant class of Land that, rather than deriving any of its attributes from information stored in the Database, can be assigned manually to produce any colour of mana when it is instantiated. This allows me to model objects that behave like Lands in simulation but do not represent real land cards, and assign them as properties of other Land objects, which is useful when modelling more complex lands. This will be detailed in \ref{sec:complexlands}.

\begin{itemize}
\item \texttt{GameCard.mandatory} - if \texttt{True}, this card must be included in the deck.
\item \texttt{GameCard.permitted} - if \texttt{False}, this card, although retrieved from the Database for player approval, was considered undesirable by the player and should not be tested or included. 
\end{itemize}

The simulation makes heavy use of two methods belonging to the Land subclass, both of which take a Game (see \ref{sec:simandsub}) as an argument:

\begin{itemize}
\item\texttt{land.live\allowbreak\_prod(game)} - given the current game state, returns the colours of mana a land can produce, expressed as a list of strings (i.e.,~["U", "W"] for a UW land.) 
\item\texttt{land.enters\allowbreak\_untapped(game)} - returns True if the land would enter untapped given the current game state, and False if not.

\end{itemize}

For examples of how these are overridden to reflect mechanically distinct cycles, see Fig. \ref{fig:vergecode} and Fig. \ref{fig:checkcode}. When a Card is returned from the ORM, the relevant GameCard subclass for it is determined via a match statement taking its attribute \texttt{Card.cycle} as an argument. 

A Spell object stores its mana cost as a list of objects corresponding to the differing costs of its faces. Each cost is a dict object relating a pip to a quantity. A Spell with one face that costs WUU2 would have \texttt{Spell.cost = [\{"W":1, "U":2, "B": 0, "R": 0, "G": 0, "C": 0, "Gen": 2\}]}.

\texttt{GameCard.wasted\allowbreak\_games} is a list attribute, usage of which will be covered in \ref{sec:eachincrement}



\begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/vergecode}
  \caption{Examle of a land belonging to the Verge cycle and the implementation of \texttt{land.live\allowbreak\_prod(game)} in that cycle}
  \label{fig:vergecode}
\end{figure}

\begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/checkcode}
  \caption{Examle of a land belonging to the Check cycle and the implementation of \texttt{land.enters\allowbreak\_untapped(game)} in that cycle}
  \label{fig:checkcode}
\end{figure}

\subsubsection{CardCollection and Subclasses}
CardCollection is an abstract class representing any Zone into which a GameCard can be moved such that it is never in multiple CardCollections. CardCollection subclasses are as follows:

\begin{itemize}
\item Deck.
\item Hand - a player's hand drawn at the start of each game, from which cards can be played.
\item Battlefield - a zone into which cards are played from the hand during a game.
\item Graveyard - a zone containing cards in a game that are no longer in play.
\item MonteCarlo - this object tests different decks and runs the Hill Climbing algorithm. It is modelled as a CardCollection in that it may be thought of as a virtual ``player'', who swaps their cards into and out of a deck before each game. 
\end{itemize}

CardCollections contain a list attribute, \texttt{CardCollection.card\allowbreak\_list}, and the specialized \texttt{CardCollection.give(GameCard, CardCollection)}, which removes a GameCard from a CardCollection's card list and adds it to the card list of another, ensuring a card is never in two zones at once.

\subsubsection{Simulation and Subclasses}
\label{sec:simandsub}
The Simulation abstract class covers objects that perform actions with a specific deck, and thus take a Deck as an attribute, and feature an overwritten method, \texttt{Simulation.run()}, which performs a simulation and assigns data from that simulation as attributes of the object. 

\begin{itemize}
\item Game - simulates a game of MTG with its deck.
\item Trial - creates many Game objects using its deck and runs them.
\end{itemize}

\subsubsection{``Lump''}
Consider the below game state: 

\[
\text{Battlefield} =
  \begin{bmatrix}
    \Land{\Forest} & \Land{\Forest} & \Land{\Island}
  \end{bmatrix}
\]

\[
\text{Hand} =
  \begin{bmatrix}
    \Spell{G} & \Spell{GG}
  \end{bmatrix}
\]

Although any individual spell in the player’s hand is castable, including the largest spell, the current selection of lands is suboptimal, as if the player had access to three green mana, they could cast two spells this turn. For this reason, rather than assessing the castability of spells, LandFill assesses the castability of ``lumps''. A Lump is an aggregation of Spell objects, which unlike in a CardCollection are not remved from any other lists when assigned. On a given turn, a Game will have a Hand, a CardCollection, and a series of Lumps, which represent different permutations of cards in that hand. \texttt{Lump.cost} is formatted similarly to \texttt{Spell.cost}, and returns the combined cost of all the Spells making it up. A Lump only stores one face of a spell; the face to be included is specified at the Lump's instantiation. \texttt{Lump.cmc} returns the sum of the CMC of all its constituent cards. 

\subsection{Initiating a Game}
At the start of each game, the deck is shuffled. The first seven cards from the Deck are moved to the Hand.

As covered in \ref{sec:mulliganheuristic}, it is not practical to implement realistic mulligan behaviour, especially since adherence to the London Mulligan approach would require strategic decision making about what cards to put on the bottom. LandFill therefore mulligans at most one time (the ``Free'' mulligan), and only if their initial hand contains fewer than three lands.

To avoid having to model the Command Zone, the Commander and Partner are added to the Hand after mulligans. 

\subsection{Running a Turn}
The below process is repeated ten times per game. Many decisions made in running an individual turn refer to the number \(L\), which is the number of lands on the battlefield at the start of the turn. The most mana accessible on any given turn, then, is \(L+1\), contingent on having a land in hand that will enter the battlefield untapped. 

\subsubsection{Untap and Draw}
Every Land object has a Boolean value, \texttt{Land.tapped}. At the start of every turn, all lands in the Battlefield object are set to \texttt{Land.tapped = False} In practice, since all spells are cast simultaneously each turn as part of a lump and tapped lands are simply treated as lands that cannot be used the turn they enter, this is not significant. Nonetheless, modelling this aspect fo game behaviour ensures a common design pattern, and may facilitate modelling other land designs. 

\subsubsection{Determining Lumps}
LandFill here makes use of the \texttt{combinations} function belonging to Python's \texttt{itertools} module \cite{itertools}. This allows it to determine all possible spell combinations in the hand. Each of these combinations are added to a Lump, to check whether they can be played with the current lands.

As this has to be done every turn to accommodate newly drawn cards, I immediately identified it as a potential runtime bottleneck. A non-mulliganned initial opening hand containing 2 lands and 6 spells (after the draw during the first turn) produces \(2^6 - 1 = 63\) combinations. While the only relevant combinations are those with combined CMC \(< L + 1\), this cannot be determined without checking individual combinations as per the Knapsack Problem (see \ref{sec:knapsack}). The simulator therefore uses the following heuristics:

\begin{itemize}
\item There is no need to search for any combinations of more than L+1 spells, as (notwithstanding spells of CMC zero, which are irrelevant for manabase optimization), there is no way to play more than this number of spells in a single turn using only lands. 
\item Any spells of \(CMC > L+1\) are ineligible, as they would not be castable in the first place.
\end{itemize}

Code profiling, carried out throughout development via the \texttt{line\allowbreak\_profiler} library \cite{lineprofiler}, shows that, with these heuristics in place, playing lumps takes up around 1/3rd  of the runtime of \texttt{game.run\allowbreak\_turn()}, making it an area for further improvement. Notably, it is \textit{not} viable here to resort to Lazy Evaluation, which in this context would mean ceasing generation of new combinations after a playable lump has been identified. Consider the following opening hand (\(L = 0\)):

\[
\text{Hand} =
  \begin{bmatrix}
    \Spell{U} & \Spell{G} & \Spell{GB} & \Land{Island} & \Land{Forest} & \Land{Swamp}
  \end{bmatrix}
\]

Were Lumps to be generated and tested lazily, the simulator would identify the spell costing U as playable, and play the Basic Island as the land for the turn. This would be a suboptimal play, however, as playing the Basic Forest would allow the same mana expenditure while also allowing the playing of a spell on turn 2 (if the Swamp were then played). 

\subsubsection{Playing a Land and a Lump}
If the Hand object contains no Lands, the playability of each Lump is determined, and largest playable Lump is ``played'' - its constituent Spell objects are moved to the Battlefield. Determining the playability of a Lump is a complex process outlined in \ref{sec:lumpplayability}.

If the Hand contains at least one Land, the Lumps umps are ranked by the value of \texttt{Lump.cmc}, in descending order. The Simulator then sequentially plays each land, determines the first (largest) lump that can be played with it on the battlefield, assigns it to the Land as a temporary variable, and then returns it to the hand. The list of land objects (\texttt{lands}) is then progressively refined by the below functions:

\begin{quote}
\texttt{allows\allowbreak\_largest = self.filter\allowbreak\_by\allowbreak\_largest(lands)}

\texttt{filtered\allowbreak\_as\allowbreak\_taplands = self.filter\allowbreak\_as\allowbreak\_taplands(allows\allowbreak\_largest)}

\texttt{filtered\allowbreak\_by\allowbreak\_most\allowbreak\_produced = self.filter\allowbreak\_by\allowbreak\_most\allowbreak\_produced(filtered\allowbreak\_as\allowbreak\_taplands)}
\end{quote}


Where:
\begin{itemize}
\item\texttt{filter\allowbreak\_by\allowbreak\_largest()} returns a list of lands capable of playing a Lump of CMC \(M\), where \(M\) is the highest CMC of any Lump.
\item\texttt{filter\allowbreak\_by\allowbreak\_taplands()} returns its input if none of the inputted lands would enter tapped this turn, and otherwise produces the ones that will. Placing this after \texttt{filter\allowbreak\_by\allowbreak\_largest()} ensures that the deck aims to play lands that enter tapped on a turn when that makes no difference to the amount of mana cast, thus preventing them from interfering at more meaningful moments later on.
\item\texttt{filter\allowbreak\_by\allowbreak\_most\allowbreak\_produced()} assesses the non-generic combined pips of every spell in the hand, and returns the land or lands which remove the largest number of these pips which are not already produced by a land on the battlefield. For example, if a hand’s spells have the combined pips R B U G G, and the battlefield contains a single swamp, then Ketria Triome (producing RUG) would have a score of 1, while Zagoth Triome (producing BUG) would have a score of 2. Lands with the lowest score are returned, and out of these lands, lands which produce the greatest variety of mana are prioritized (i.e.,~for a hand requiring G and U, a BUG land would be prioritized over a GU land)
\end{itemize}

A land is played from the returned lands at random, increasing the value of \(L\) by one, and a lump is played with values as close to \(L\) as possible. When the Lump is played, each land used in its casting is set to to \texttt{land.tapped = True}. If relevant, as in the case of Fetch Lands (see \ref{sec:fetchlands}), the Land is informed what color of mana it will be producing, as per the mapping returned by the Linear Assignment Function outlined in the following section. 

\subsubsection{Assessing Lump Playability}
\label{sec:lumpplayability}
The question of whether a given Lump can be played with a given set of lands is non-trivial. This is partly to do with the inbuilt complexity of some lands; complex designs such as Filter Lands, Check Lands and Dual-Faced Lands are discussed in \ref{sec:complexlands}. However, even setting these aside, the fact that some lands produce many colors of mana while others produce few or one means that a mapping between land and pip must be established such that multicolor lands are not ``wasted'' on pips whch are already well served by lands offering fewer colours. Several approaches to this were explored during development.

My initial approaches involved generating a list of all combinations of mana that the Lands in the Battlefield object could produce. If an entry on this list included all pips in \texttt{Lump.cost}, then that Lump is playable. The runtime bottleneck that this produces should be quickly obvious, especially as there are no comparable heuristics to those used when determining Lumps. Consider the below battlefield:
\[
\text{Battlefield} =
\begin{bmatrix}
  \Land{BUG} & \Land{BUG} & \Land{UG} & \Land{UG} \\
  \Land{UG}  & \Land{GB}  & \Land{GB} \\
  \Land{\Island} & \Land{\Island}
\end{bmatrix}
\]

In this case, there are \(3 * 3 * 2 * 2 * 2 * 2 * 2 * 1 * 1 = 288\) combinations, which must be re-calculated on every played land. While lazy evaluation is an option here, it does not save much time, as any half-completed lazy calculations must be needlessly re-commenced if another lump is played before a new land is played. 

 To save recalculating combinations each turn, my initial solution was to store the list of combinations as an attribute of the Battlefield Object. Whenever a new land, capable of producing \(N\) different colours was played, its first colour would be added to all existing combinations. The Battlefield would then generate \(N-1\) shallow copies of each combination, adding a different colour produced by the new land to each. Even without new combination generation, however, simply iterating through the existing combinations to update them proved far too slow. It additionally made it impractical to simulate removing lands from the battlefield. Some lands, such as Verge Lands and Filter Lands (see Fig. \ref{fig:cycles} in \ref{sec:balancinglands}), produce different colours of mana depending on what other lands on the battlefield. This means that, when determining what land to play, the simulator must account for not only what mana that land can produce, but what mana it enables pre-played lands to produce. The easiest way to simulate this is to ``play'' each land, assess lump playability with it on the battlefield, and then return it to the hand before testing the next land. 
 
 A more promising solution was to establish a function capable of generating all combinations of colored mana from a an input set of lands, and memoize the output of this function. In memoization, the output of a function from a given set of arguments is stored in a cache, and a new output is calculated only if those arguments are not already cached \cite{whatismemoization}. In Python, this cache can be persisted across sessions via the Pickle library \cite{pickle}. Because any two untapped lands that produce (for example) UB are functionally identical in this context, the number of combinations to memoize appears at first comparatively small, as each land can be canonicalized only as the colours it produces. Moreover, basic lands do not need to be canonicalized, and simply reattached to each permutation afterwards (see Fig. \ref{fig:canonicallyidentical}).

 \begin{figure}
    \centering
  \includegraphics[scale=0.25]{card_images/CanonicallyIdentical.jpg}
  \caption{Two potential arrangements of lands that can be canonicalized identically}
  \label{fig:canonicallyidentical}
\end{figure}

This functioned acceptably for a 3-colour deck. Memoization via Pickle produced a .pkl file of around 1000 megabytes. However, when I trialled a 5-color deck, this expanded by a factor of five (and may have continued to expand). The cache in memory, meanwhile, became so lengthy that a single cache miss took multiple seconds, and even a comparatively small number of cache misses increased the runtime, at this stage in development, from running 1000 games in about five seconds to running that same number over forty minutes. 

Ultimately, I settled on modelling the question as a Linear Assignment Problem using SciPy’s \texttt{linear\allowbreak\_assignment()} function \cite{scipy}. Koopmans and Beckman \cite{koopmans1957assignment} set out the Linear Assignment problem in the following layman's terms: paraphrasing, if a set of factories are to be built on a set of plots of land, and the suitabilities of a given plot to a factory's production processes means that each factory will return a specific profit at a specific plot, how can plots be assigned to factories to maximize the overall profit? In this context, it is helpful to invert the example. If a factory incurs a specific \textit{cost} at a specific plot, how can plots be assigned to minimize costs?

Plots of land are here equivalent to Land objects, while the factories are equivalent to the pips of a Lump's cost. \texttt{Lump.cost} is here reformatted into a list of strings corresponding to the number of each pip (treating the ``Gen'' key in the dict object here as a pip). The list is then given any number of strings that read "None", to ensure that the length of the list is equal to \(L\) (ensuring, with reference to the above example, that the Linear Assignment solution holds even if there are fewer factories than plots). Any lump with a total CMC greater than \(L\) is discarded from consideration. ``Costs'' are then set for each Land, using \texttt{Land.live\allowbreak\_prod}, like so:


\begin{center}
\renewcommand\arraystretch{1.3} % more row height
\begin{tabular}{| m{12em} | m{12em} | m{10em} | m{5em} | m{5em} |} 
 \hline
  & \makecell[l]{A pip in \\ \texttt{Land.live\allowbreak\_prod()}} 
  & \makecell[l]{A pip not in \\ \texttt{Land.live\allowbreak\_prod()}} 
  & ``Gen'' & ``None'' \\ 
 \hline\hline
 A Land for which \texttt{Land.tapped = True}  & 9999 & 9999 & 9999 & 2 \\
\hline
 A Land for which \texttt{Land.tapped = False} & 2 & 9999 & 2 & 2 \\ 
\hline
\end{tabular}
\end{center}

\texttt{SciPy.linear\allowbreak\_assignment()} then returns a mapping of lands to pips that minimizes this total cost. If the total cost is less than 9999, the Lump is playable. While this did not perform better than the memoization method for a 3-color deck, time penalties for 4-colour and 5-colour decks became negligible. 

\subsection{Concluding a Game}
When a game is concluded, all cards owned by the Hand, Battlefield and Graveyard are returned to the Deck. Performance metrics for the game are set as attributes of the Game object, for querying by the Trial and MonteCarlo objects. 

\subsection{Heuristics}
\label{sec:simulationheuristics}
For the initial development covered in this writeup, the below heuristics have been adopted; all offer an opportunity for future improvement of the product.

\subsubsection{Multiple-Faced Cards and Alternate Casting Costs}
Spells are only cast via the mana cost of their first playable face, and not via any alternate mana costs. As mentioned, many spells are in fact two spells, either of which may be played; many more cards have alternate casting costs listed in their textboxes, where casting them for a different cost changes the behaviour of the card on cast. Ideally, LandFill would treat these both as seperate spells when assinging combinations in the hand, and denote a card as cast if either of its options are played. However, in addition to the difficulty of parsing the textboxes of cards with multiple costs, representing some cards in the hand as two cards which cannot both be played in the same Lump adds an extra layer of complexity to Lump creation, and has not been implemented at this stage.

\subsubsection{Spells with X in their Mana Cost}
A spell with mana cost GX may be cast for one Green mana plus any amount of generic mana, usually acruing more value to the player for a higher value of X. This is complicated to include in Lump combinations; moreso in the case of cards with multiple values of X (a card costing XX being includable in any lump that leaves an even quantity of leftover mana). Therefore, X is always assumed to be 1 generic mana. 

\subsubsection{Strategies for Future Turns}
The ordering of \texttt{filter\allowbreak\_by\allowbreak\_largest()}, \texttt{filter\allowbreak\_by\allowbreak\_taplands()} snf \texttt{filter\allowbreak\_by\allowbreak\_largest()} encourages the Simulator to play a land that enters tapped on a turn when this does not affect mana expenditure. This allow for some forethought, relevant to situations such as the sample comparison between the Battle and Slow lands in \ref{sec:balancinglands}. However, there are some more niche situations in which this order of priorities does not apply. Consider the below hand with \(L=0\):

\[
\text{Hand} =
  \begin{bmatrix}
    \Land{BW} & \Land{\Island} & \Land{\Island} & \Spell{BBWW4} & \Spell{UU}
  \end{bmatrix}
\]

Since the BW land removes more colours of mana from the hand, it would be played by the simulator. However, the spell requiring BW had a CMC of 8 and will not be played for some time; playing the Basic Island, however, would allow playing the UU spell on the following turn. Since it is necessary for the Simulator to assess the castability of Lumps rather than Spells, planning for future turns is difficult, and indeed situations like this may only be coverable via the progressive addition of heuristics. A future refactor may replace the hand object with a ``queue'' of spell combinations of progressively higher mana costs, adding each newly drawn card to this queue. 

\subsubsection{Ramp and Draw Spells}
Many spells draw additional cards, while some provide additional mana. Doing so would potentially be a very fruitful area of development, and will be discussed in the conclusion. However, in addition to the difficulties of coding ramp and draw spells, introduction of these factors bring complex strategic considerations - it may, for example, be advisable in some situations to spend less than \(L+1\) mana on a ramp spell to allow for greater overall expenditure later; it is also typically advisable to play draw spells before playing lands if possible, to see if a superior land is added to the hand. 

\subsubsection{Bond Lands}
Bond Lands are a cycle of two-colour lands without basic landtypes. They enter tapped unless a player has two or more opponents, making them extremely strong in Commander. Bond Lands, in the Simulator, always enter untapped. This not wholly representative of real gameplay, as they may enter tapped if soem players have already been defeated, but this is not something that can be factored in within the limitations of Goldfishing. 

\subsection{More Complex Lands}
\label{sec:complexlands}
Some land cycles required significantly more complex simulation logic, outlined below. Mechanics for the respective cycles are either detailed below or can be found in Fig. \ref{fig:cycles} in \ref{sec:balancinglands}. 

\subsubsection{Fetch Lands}
\label{sec:fetchlands}
During assessment of Lump playability, a Fetch Land (instantiated as a FetchLand object) is considered to produce mana of colour \(M\) mana if:

\begin{itemize}
\item The deck currently contains a land for which it can search that can produce \(M\)
\item That land would return \texttt{land.enters\allowbreak\_untapped(game) = True} for the current game state.
\end{itemize}

The search itself happens is set when the FetchLand is tapped for mana. The FetchLand calls the \texttt{game.filter\allowbreak\_by\allowbreak\_most\allowbreak\_produced()} method from the current Game object to narrow down the lands it is capable of fetching, adding an extra argument to specify that it the method must also account for pips in the hand currently unaccounted for by lands in the \textit{hand}, rather than just on the battlefield. This encourages the FetchLand to make new colours of mana available. After this, the FetchLand is moved to the Graveyard. If tapped for ``None'' mana, a FetchLand will search for a land that will enter tapped, if one is available in the deck.

While production of W, U, B, R, or G by a FetchLand has the standard cost of 2 during Linear Assignment, production of ``Gen'' is weighted at cost 1, and ``None'' has cost 0. This means that \texttt{SciPy.linear\allowbreak\_assignment()} uses the FetchLand to pay for a ``Gen'' or ``None'' pip if possible. This prevents the FetchLand from being forced to search for a colour it does not need to in order to pay a generic cost, and allows it to, as much as possible, find the best land for the current hand rather than just for the spell.

To ensure that any FetchLands search for Lands even on a turn when no Lump is played - allowing them to remove from the deck lands that will enter tapped, as per the scenario in \ref{sec:balancinglands} - an additional ``Null Lump'', containing no Spells, is created each turn, forcing the Fetch Land to always provide at least one ``None''. 

\subsubsection{Filter Lands}
A Filter Land (instantiated as a FilterLand) produces C and has two SubLands (see \ref{sec:gamecards}), each of which may produce either of the Filter Land's colours. In order to account for a situation in which multiple FilterLands may pay for the abilities of others, the following recursive algorithm is used:

\begin{enumerate}
\item If a Lump is castable on a battlefield containing FilterLands while tapping those FilterLands for C, it is cast as normal.
\item If not, FilterLands are placed into a new list, $\underline{L}_f$. A permutation of this list is lazily generated via \texttt{itertools.permutations()}, from first Filter Land \(F(1)\) to \(n\)'th Filter Land \(F(n)\). 
\item A new list, $\underline{L}_nf$, containing all non-Filter Lands is generated, including the subset of lands capable of paying \(F(1)\)'s cost, \(P(1) ... P(n)\). \(P(1)\) is removed from $\underline{L}_nf$, and the SubLands of \(F(1)\) are added in its place. 
\item Repeat steps 2 and 3, starting with $\underline{L}_nf$ each time, until all FilterLands have been either replaced with their SubLands, or tap for colourless if there is no land capable of paying for them, with \(F(n)\) being the final one so replaced.
\item If the Lump is castable with the resulting list of lands and sublands, cast it. 
\item If it is not, return the land \(P(1)\) that had been removed at the recursion depth when the sublands of \(F(n)\) had been added, and remove \(P(2)\). Assess the castability of the Lump again, casting it if possible.
\item If no lands \(P(1)\)-\(P(n)\) in the list at the recursion depth of \(F(n)\) allow for the lump to be played, return to step six for the list at the recursion depth of \(F(n-1)\). 
\item If the recursion depth of \(F(1)\) is reached, return to step 2 generate a new permutation with new values of \(F(1)\) and \(F(n)\)
\item If all permutations of $\underline{L}_f$ have been tried, the Lump is not castable.
\end{enumerate}

Although this requires testing a large number of permutations if a Lump is not castable, the rarity of having a large number of FilterLands simultaneously in play means that this does not incur a noticable runtime penalty.  

\subsubsection{Dual-Faced Lands}
A Dual-Faced Land has two faces, each of which are playable, and each of which tap for exactly one colour of mana. All Dual-Faced Lands enter the battlefield untapped. The DualFacedLand objects is assigned two SubLands, each capable of producing one of its colours, and an attribute, \texttt{DualFacedLand.committed}, initialized to \texttt{None}. When tapped for ``Gen'' or ``None'', if \texttt{DualFacedLand.committed = None}, one of the DualFacedLand's SubLands is selected via \texttt{Game.filter\allowbreak\_by\allowbreak\_most\allowbreak\_produced()} in a similar manner to FetchLands, and \texttt{DualFacedLand.committed} is set to that SubLand. When tapped for coloured mana, \texttt{DualFacedLand.committed} is set to the SubLand capable of producing that colour. If \texttt{land.committed != None}, the land is tapped as though it itself were the SubLand returned by that attribute. The Battlefield object sets \texttt{DualFacedLand.committed} to \texttt{None} when it returns it to another zone. Like FetchLands, a DualFacedLand has a lower weighting to force \texttt{SciPy.linear\allowbreak\_assignment()} to map it to ``Gen'' or ``None'' mana if possible when a Lump is played.

\subsection{Simulator Verification Testing}
Testing of the Simulator was done via optional \texttt{samplehand = []} and \texttt{sampletopdeck = []}arguments passed to \texttt{Game.run()}. Both are arrays of card names as strings, which, if included, are extracted from the deck and placed either in the hand or at the front of the deck object after it is shuffled. This allowed me to watch the Simulator play sample hands and ensure behaviour was as expected.

\section{The Optimizer}


\section{The Interface}
The Web Interface component largely consists of LandFill's frontend, but also includes the InputParser class in the backend. Both are outlined below.

\subsection{The Input Parser}
In my initial user-research, subjects desired that LandFill support ``round trips''. this means that LandFill can receive a list of cards in the formats output by known online card databases, and output cards in the same format. For this purpose, I created the InputParser class, of which one is created at the start of each session. Test users identified four databases: TappedOut, Moxfield, Archidekt and Deckbox. Since Tappedout and Moxfield both allow for lists to be copied from the deck homepage rather than from the formatted export panel, and Moxfield and Deckbox take inputted cards in the same format, this means that the InputParser has been designed to distinguish between six possible input formats, and return three possible output formats. 

Since Tappedout and Archidekt both support categorization of cards (i.e.,~draw, ramp, win strategy, enemy card removal) with custom lables, the Input Parser stores all cards in a Dict object corresponding to each category, using a default key if none are specified by the user. On completion of the Monte Carlo process, the InputParser a total of six string objects: for each database format (Deckbox and Moxfield being identical), it returns the total decklist including lands (structured into categories if permitted by the database and provided by the user), and the list of provided lands. 

\subsection{App Layout}
In the intial mockup drafted in \ref{sec:initialusertests}, LandFill consisted of a single homepage. Users in the think-aloud evaluation generally did not understand the difference between the button that confirmed their nonland inputs and the button which commenced the MonteCarlo. On redrafting, I split the design into four pages, outlined below and displayed in Figs. \ref{fig:deckinput}, \ref{fig:preferences}, \ref{fig:progress} and \ref{fig:output}

\subsubsection{Deck Input (Fig. \ref{fig:deckinput})}
This page allows the user to list details kept constant throughout the session, i.e.,~cards inputted by the user, and the currency in which the session is to list card prices.The option to overwrite inputted lands with a new manabase was added in response to users in the Think-Aloud evalution who instantly copied a completed deck from a database into LandFill and manually removed lands from the list. 

\begin{figure}
    \centering
  \includegraphics[scale=0.35]{card_images/frontendpageone.jpg}
  \caption{Deck Input page. }
  \label{fig:deckinput}
\end{figure}

\subsubsection{Preferences (Fig. \ref{fig:preferences})}
\label{sec:preferences}
The Preferences Page allows users to customize what lands are candidates for inclusion in the deck. The initial mockup had allowed players to exclude lands or whole cycles via a series of tickboxes, but during the Think-Aloud evaluation, users had found this both overwhelming, given the sheer quantity of lands, and unintuitive, as they did not always know what a given cycle did. Moreover, as LandFill can run more quickly if more cards are deemed by the player to be mandatory for inclusion, my design philosophy was to make it is easy as possible for to mark favoured cards, rather than placing the burden of them to list mandatory lands as part of the deck input. This required me to replace the binary input of a land being unchecked (excluded from consideration) or not with a three-way system, by which players could mark cycles as mandatory, possible or forbidden. I addressed all of these issues via a trio of drag-and-drop boxes from the React-Beautiful-DND library \cite{reactbeautifuldnd}, while also adding a side-panel element which would show a sample card and card list from a draggable cycle object on mouseover. 

Since some lands may behave identically as manabase components but may have additional mechanics that a player may prefer, I created two additional drag and drop boxes, each of which corresponds to a category of identical lands. The frontend does not support dragging between these boxes; rather, they are used to prioritize these lands according to thhe user's preference. Information from this input creates a session-specific amendment to the prioritization hierarchy used in the backend LandPrioritization object. For the preferencse set in Fig.\ref{fig:preferences}, LandFill will only trial a Bicycle Land if the Typed Dual Land of its colours is either already in the deck or is excluded from consideration (both Bicycle and Typed Dual Lands being cycles of two-colour taplands with basic landtypes).

The panel of preferences still set via tickboxes and numerical inputs above the drag-and-drop column are based on comments made by users during the initial mockup testing. Recall from \ref{sec:knapsack} that players did want to factor price considerations into their choices. While it is impractical, as detailed, to allow the player to set a maximum price for the manabase, LandFill gives players the option to exclude all land cards above a certain price. I consider this to be a reasonable, if imperfect, proxy.

Given the complexity of the interface, it is worth touching on the underlying logic. A common theme among test subjects in pre-development evaluation was a lack of a single consistent plan or criterion determining what lands are included, with players generally expressing a range of broad preferences often caveatted by circumstance - price, for example, not being a factor if a player already owned an expensive land, even if they were not interested in buying more from the cycle. As another example, a subject who expressed considerable hostility to any land which only ever enters tapped admitted a fondness for the Surveil Land tapland cycle due to its utility effect. I felt it necessary, consequently, to always give players an override option. Individual cards can be added to consideration despite the status of the cycle as a whole via the tickboxes in the side panel, while use of any of the ``Exclude from Consideration'' Filters, which moves multiple drag-and-drop objects to the ``Exclude'' column, does not prevent any individual items so moved from being dragged \textit{out} of that column. Since this means that a cycle or individual card may be categorized either by dragging and dropping or via a filter or override method, the state of any given land is always mapped to one of two ``positiveArrays'', \texttt{positiveArrays.include} or \texttt{positiveArrays.consider}. If it is excluded via any method, it is put in one or more of several ``excludeArrays'', corresponding to the frontend input used to exclude it (i.e.,~whether it was excluded with its whole cycle via the Drag and Drop panel, whether it was excluded for being an off-colour Fetch Land...). When the simulation is run, all lands in any excludeArray are marked as excluded, and all others are marked as either mandatory or possible, depending on which positiveArray they are in. 

Information about the methodology and criteria used by LandFill, including an explanation of the \(P\) metric (see \ref{sec:initialanalysis}), is contained in the labelled panels, and is available on clicking. 


\begin{figure}
    \centering
  \includegraphics[scale=0.35]{card_images/frontendpagetwo.jpg}
  \caption{Preferences page, zoomed out to show all content; the rankings panels would normally be offscreen. The side-panel remains stationary as the user scrolls.}
  \label{fig:preferences}
\end{figure}



\subsubsection{Progress (Fig.\ref{fig:progress})}
\label{sec:progress}
 Although initial drafts stayed on the Preferences page while the Optimizer runs, I felt that this was inappropriate for a runtime greater than one minute - I did not want the users to think that the program had simply crashed. Moreover, during mockup testing, several users expressed a desire to see the logic being used by the backend, as this would make the results more trustworthy. Although the Preferences Page contains a contextual explanation of the Hill Climbing algorithm, the use of a Progress page allows for this to be more clearly demonstrated: each step in the Hill Climb increment is printed to screen. At the conclusion of the Hill Climb algorithm, the Output page loads automatically. 

\begin{figure}
    \centering
  \includegraphics[scale=0.55]{card_images/frontendpagetwopfive.jpg}
  \caption{Progress page}
  \label{fig:progress}
\end{figure}

As this feature was added late in development, it is done crudely, by polling the backend every two seconds for new updates. 

\subsubsection{Output (Fig.\ref{fig:output})}
 The core feature of the Output Page is the textarea which shows the recommended decklist, which can display either the lands added to the deck or the entire decklist for easy export. When the entire decklist is displayed, if the input format included custom card categorizations (i.e.,~from TappedOut or ArchiDekt), these are re-added here, with new lands being added either under a new category, ``Lands'', or under an existing category if one was detected by the InputParser that contained other land cards at input. 

The left hand panel allows users to use the same mousever interface as was employed on the Preferences Page to examine the cards added to their decklist. One subject, during initial user research, had asked to see the lands ranked by performance, as this would inform both their choice of ramp spells and tell them what lands they should replace on the printing of a new cycle.  

\begin{figure}
    \centering
  \includegraphics[scale=0.35]{card_images/frontendpagefour.jpg}
  \caption{Output Page}
  \label{fig:output}
\end{figure}

\section{Validation Testing}
\subsection{Methodology}
I divided my post-development Validation Test into three broad sections, listed below. These were administered to five test subjects.

\subsubsection{Effort Testing via TLX}
LandFill adds value if it is able to create, with a user's input, a manabase for a deck that they consider to be of equivalent or superior quality to a manabase they could produce with an equal or lesser amount of time and effort. Measuring the quality of output against the time and effort required to obtain it can be done via the Nasa Task Load Index (TLX). The TLX is a standardized questionnaire with a long history of industry usage. It uses six questions, each ranked out of 100 in increments of 5 to determine a subject's perception of how much effort they expended in service of a task, as well as how satisfied they are with their performance of that task \cite{bustamante2008measurement}. The questions are as follows \cite{nasatlxhomepage}:

\begin{enumerate}
\item How mentally demanding was the task?
\item How physically demanding was the task?
\item How hurried or rushed was the pace of the task?
\item How successful were you in accomplishing what you were asked to do? (note that for this question, a score of 0 denotes a feeling of perfect success, not total failure)
\item How hard did you have to work to achieve your level of performance?
\item How insecure, dsicouraged, irritated, stressed and annoyed were you?
\end{enumerate}

A total score can be obtained by summing these scores and divided by 6, although the ``raw'' scores can be valuable as well.

I provided subjects with two incomplete Commander decks, a BUG deck and a WUR deck, both of which consisted solely of 62 nonland cards, and asked them to choose 38 land cards for each. Participants were allowed use of any existing deck-building apps, including Archidekt and Mana Gathering, for the WUR deck, and were asked to complete the BUG deck using LandFill. I timed participants in both tasks. After each task, subjects completed a TLX. This allowed me to compare a subject's perceived effort when building a manabase by hand to their perceived effort when using LandFill.  

Although in some TLX analyses the user is asked to weight the six dimensions by significance to their perceived effort, there is evidence that this negatively impacts accuracy \cite{bustamante2008measurement}. As I am interested in comparing raw scores, I am not necessarily interested in weighting anyway, so did not do this.

\subsubsection{Serendipity Testing}
It is impractical to test the quality of manabases recommended by LandFill by simply judging the win/loss rate of decks built around its suggestions. MTG games take time, and introduce many confounding variables, including player skill. Instead, it is more feasible to test simply whether a user is satisfied with the manabase recommended. This is touched on in the TLX, which asks users to rate their ``performance'' at the task (in this case, building a good manabase with LandFill), but warrants dedicated investigation. Problematically, however, several subjects during User Research expressed a desire for an app that would recommend them lands that they might not have thought of otherwise (see \ref{sec:personas}). Therefore, it is important to test the degree to which, insofar as LandFill is making distinct decisions to a human player, those decisions are welcomed by the human player. 

I assessed this via the following method. After users had completed both TLX exercises, I used the preferences they had set for LandFill in generation of the BUG deck to generate an alternative list of lands for the WUR deck, and used a simple Python script to categorize lands (here just string representations of the land name) into:

\begin{itemize}
\item Lands included in both decklists.
\item Lands selected by the user not selected by LandFill.
\item Lands selected by LandFill not selected by the User.
\end{itemize}

I identified lands final two lists, and asked the user's opinion on LandFill's decision to include or exclude each, grouping into cyles where possible for simplicity. Basic Lands were included for the script with their numbers - i.e.,~if a player chose 4 Basic Islands, and LandFill chose 6 Basic Islands, than the second list would include ``4 Basic Islands'' and the latter would include ``6 Basic Islands.'' 

While this does not map cleanly onto any standardized testing mechanism, it is inspired by the concept of ``Serendipity'', which is used in the analysis of recommender systems such as the algorithms of sites like Spotify and Instagram. Serendipity compares the ``unexpectedness'' of recommended items - the similarity between recommended items and a user's previous item interactions - with their ``relevance'', i.e.,~whether the user interacted with them after recommendation \cite{SerendipityRecSys}. However, as these more formalized metrics are designed for large platforms that accrue preference data over time, I significantly adapted them. I also treated them as a prompt for discussion, and thus a source of qualitative information, rather than as a numerical metric.

\subsubsection{Semi-Structured Interview}
In addition to the discussion prompts provided by the Serendipity Testing, I asked two additional questions:

\begin{itemize}
\item Do you have any suggestions for improvements or new features?
\item Do you have any other general thoughts on LandFill? 
\end{itemize}

\subsection{Results}
\subsubsection{TLX Data Analysis}

\begin{figure}
    \centering
  \includegraphics[scale=0.45]{card_images/TLX_scores.jpg}
  \caption{Scores and timings from the two TLX analyses, and the differences between these values.}
  \label{fig:tlxdata}
\end{figure}

Scores from the TLX questionnaire are broken down by participant in Fig.\ref{fig:tlxdata}. In all cases, use of LandFill resulted in:

\begin{itemize}
\item A lesser overall perceived workload.
\item An equal or higher satisfaction with performance of the task - i.e.,~with the quality of the manabase generated.
\item A reduction in time to complete. 
\end{itemize}

Only participants 4 and 5 felt LandFill required more effort in any of the dimensions, and these were also the participants who felt that LandFill had increased their performance by the highest margin. Only participant 3 recorded no difference between their performance with LandFill and without it. This participant also saved by far the most time through use of LandFill, suggesting that this user naturally takes more time to build decks to a higher standard. This strongly vindicates my thesis that LandFill is a useful tool for deckbuilders. 

\subsubsection{Serendipity Test Analysis}
Predictably, LandFill always recommended different quantities of each basic land than users. Since the deck under test was WUR, each generated manabase included a number of Island, Plains and Mountain cards. Users said that, when choosing a manabase, they had either divided land slots equally among the basic, or intuited the number based on the proportions required by the deck. All save one were happy to accept LandFill's estimates over their own. Because the test decks are stored on TappedOut, which lists in a pie-chart the proportions of colours required by the deck, it is also notable here that even the more thoughtful players here were making decisions based on more information than they may have had access to when deckbuilding normally. In initial user research, many participants said they determine complete decklists, including manabases, before uploading them to TappedOut or an equivalent database.

The one subject who \textit{did} favour their own quantity of basics over LandFill used far fewer basics than LandFill recommended. This is because, while users were asked not to consider utility lands, in practice, the definition of a utility land is vague; some utility lands do generate coloured mana. This particular subject chose single-colour utility lands that entered untapped over basic lands, and said that this was a general preference of theirs. 

If a nonbasic land was included in LandFill's list but not in the player's, the player when asked cited one of several possible explanations, including: 

\begin{itemize}
\item The player had forogtten about this land.
\item The player habitually did not think about this land.
\item This land cycle incurs a life point penalty that the player typically shies away from.
\end{itemize}

Only the third case reflects a negative assessment of LandFill's suggestion; in these cases, the player did not use the option to dismiss these lands from consideration. Given the absence of a holistic score assessing the life point damage incurred from the manabase, it may be necessary to include a more visible option to exclude lands based on life point investment in future versions.

If a nonbasic land was included in the player's list but not LandFill's, this was usually because it was a land that was not coded into LandFill. Typically, this was because it was a land that is not contained in a cycle - this is common for WUBRG lands. Currently, the only non-cycle land supported by LandFill is the Command Tower. Future iterations would thus likely benefit from inclusion of more non-cycle lands. While this is trivial to include in the backend, as these lands typically behave quite simply, it is potentially more complicaed in the frontend, the layout of which is based on cycles. 

\subsubsection{Semi Structured Interview Response Analysis}
Since much of the interview focussed on potential new features and fixes, the bulk of the data obtained here will be covered shortly in \ref{sec:areasforimprovement}. However, I will briefly touch here on general thoughts. While social desirability bias is a factor here, four out of the five participants described LandFill as a useful service. Subjects highlighted its relevance not only to card selection but to formatting and card recollection, as it prevented spelling mistakes and did not accidentally suggest lands in the wrong colours. Several subjects commented that even if LandFill had not saved them time overall, automation of the process made it feel much less temporally demanding. Compared to the initial mockup, subjects found it much easier to mark cards for inclusion and exclusion, as LandFill now informs them of the behaviour of each cycle via the displayed card image. 

Users, unfortunately, did experience some bugs in using the system. The InputParser did not account for formatting differences between search engines, and the toggle switch to include or remove nonland cards from the final deck output behaved temperamentally. Additionally, no user tested made use of the ability to rank mechanically equivalent lands; players either removed all (or all save one) rankable cycles from consideration or did not put much thought into which lands they wold prefer. 

In several cases, it was clear in the final output that the results were only approximate. One user noted that one individual Basic Mountain performed far higher in the output than all others did, and higher than several dual lands. A second noted that, in the BUG deck, an ``on-colour'' Fetch Land, searching for an Island and a Forest, ranked below two ``off-colour'' fetches, which could search respectively for Swamps and Plains and Forests and Plains. While, as outlined, these Fetch Lands are still useful in the deck, as they can fetch Shock Lands, they should be considered strictly inferior in context to on-colour fetches. 

In both these specific cases, this can be addressed without fundamental alteration of LandFill's methods. Regarding the latter, Off-Colour Fetch Lands can be ranked below On-Colour Fetch Lands in the LandPrioritization. Regarding the former, while it is useful during each Hill Climb increment to score each Basic Land seperately, as it prevents nonbasic lands from being ejected prematurely, it may be wise to normalize all basics of each colour in the final output. However, in a general sense, it is likely worthwhile to stress to users more clearly that LandFill's output is not totally proscriptive.

\subsection{Additional Features}
\label{sec:areasforimprovement}
\subsubsection{Persistant Preference Storage and Links with Existing Databases}
\label{sec:persistantstorage}
Storage of user data between sessions was determined early in development to be wholly outside the scope of the prototype. Two testers bought it up as an immediate next step. In both cases, the subject felt it was important not simply to persist user preferences, but fully track the lands in a user's collection. This could be done either by allowing LandFill to store a list of lands that a user owns, or by allowing LandFill to link with a user's account on another card library database. In both cases, this would require users to be able to create a secure account with LandFill. 

\subsubsection{Flexible Runtimes and Accuracy}
Several users commented that, as the bulk of LandFill's runtime required no user input, it could afford to take longer than it currently did. Participant 3, notably, spent almost twice as much time producing their manabase as LandFill spent to conduct 10000 simulated games and 2000 card tests per run as detailed in \ref{sec:eachincrement}, which resulted in an increased consistency of 10 percentage points compared to the amount of simulations currently run. They outlined a desirable use-case for LandFill where they specified a high degree of optimization consistency, and then ``left it running while [they] had [their] lunch''. 

In the future, LandFill could offer users one of several accuracy level options, with projected runtimes for each. This would require significant testing in its own right, however, as in order to let users make an informed choice, I would need to analyse the average Jaccard index improvement as a function of runtime increase across 2, 3, 4 and 5 colour decks. 

\subsubsection{Additional Formats and MTG: Arena Support}
Users suggested the inclusion of other formats as a potential next step. Players also suggested that LandFill be formatted for use with MTG: Arena. 

Given that Commander was the most popular format among my initial test subjects, and one designed for casual play, I stand by my decision to restrict initial development to this format, as it allowed me to focus on the core functionality of LandFill without worrying about handling format specifications. However, if desirable, this expansion is possible without fundamental alteration of this functionality.

\subsubsection{Frontend Formatting Improvements}
While the completed frontend was considered by subjects to be easier to navigate than the mockup, subjects still identified several problems. These are listed below.

\begin{itemize}
\item Since the ability to select or deselect individual lands within a cycle was accessible on the side panel via mouseover, players had to move their mouse carefully from the cycle label to the panel to avoid accidentally brushing over another cycle. Ideally this would be replaced by a drop-down menu accessable over the cycle label itself.
\item Currency is set globally on the first page. Subjects requested the ability to change it on subsequent pages, if they missed it on the first one. 
\item Card prices were taken from ScryFall as downloaded by Scrython. One subject pointed out, however, that this meant that LandFill only displays the lowest price for a given land, which may be an undesirable printing. Since cards are sold on a second-hand market, and price differs by printing, prices should be given as a range, not as a single value. 
\item Although a FAQ on the Preferences page outlines the performance metric used, this should be re-stated on the output page in case the user ignored it. 
\item One subject pointed out that, if a cycle is moved to the excluded column, but some individual lands were ticked within it, the label of the cycle should be represented differently within the column to show that it is not entirely excluded.
\end{itemize}



\section{Conclusion}
\subsection{Summary}
In this thesis, I have outlined why spontaneous generation of an optimized manabase is complicated. I broadly outlined LandFill's use case, as a flexible adaptation of the testing scripts developed by Frank Karsten so that they could be used to automate card selection for an individual deck, rather than just determine deckbuilding best-practices. I then illustrated how I organized a local database of MTG cards, and equipped it to be continously updated in response to future sets. 

Armed with this database, I have outlined how I used Python to implement an extremely stripped down MTG simulator, how to encode behaviour that effectively utilizes mana, how to model the behaviour of land cycles, and the heuristics accepted here during initial development. I then demonstrated how to use the output of these simulations in a Hill Climbing algorithm to explore the search space of possible manabases in a way that balances runtime with accuracy. I then illustrated the frontend design that would allow this apparatus to be used by lay MTG players, and how it encourages them to simplify the job of the Optimizer component by specifying lands they do and do not want. 

I finally outlined preliminary test results from this system, and how they strongly suggest that LandFill, subject to de-bugging, is already capable of streamlining the deckbuilding process, making it quicker and easier to assemble manabases to a higher standard. I then outlined features that users proposed, with some examination of how they might be implemented. 

Throughout this writeup, I have highlighted where LandFill is a limited product. Its optimizations, even aside from the susceptibility of Hill Climbing algorithms to local maxima, are approximate and not wholly consistent, and there are subtleties of gameplay that it simplifies even within its simplified goals (maximize mana expenditure) as an automated player. While these do not impede its utility, as user testing demonstrates, I will conclude here by exploring some areas of development where, with hindsight, I may have made different decisions.

\subsection{Self Assessment}
\subsubsection{Use of Python as Backend}
\label{sec:python}
Recall from \ref{sec:eachincrement} that, measured by average Jaccard Index, MonteCarlo runs that took less than ten minutes for a three-colour deck only shared around 70\% of lands in common. This suggests that, with use of the simulator as outlined here, a single optimum - even a local maximum - cannot be reliably reached in a usable timeframe, although user testing has expanded my perception of what a usable timeframe may be. However, the increase in Jaccard Index when longer timeframes were permitted does suggest that there is room to improve LandFill simply by increasing the speed of the Simulator.

Code profiling was conducted repeatedly through development via the \texttt{line\allowbreak\_profiler} library. It identified the main contributor to the Simulator's runtime the generation of Lumps and the assessment of Lump playability; while I adapted both processes several times to reduce this, and experimented with higher-performance libraries such as SciPy, there remains a runtime floor based on my current algorithm. Because of this, Python may not have been the best choice of a backend language. I chose Python due to its wide use in webdesign, but it is not considered a high performance language. A future restructure may involve translating the backend into a higher performance language such as C++.

An intermediate solution may be to refactor around a piplining library such as Joblib \cite{joblib} to allow multiple games to be run simultaneously. While I did explore a joblib-based implementation, the required deepcopying of deck and card objects immediately counteracted any runtime improvements. If it is feasible, it would have had to inform the structure of the objects at a much earlier stage of development. 

\subsubsection{Use of Hill Climbing Algorithm}
Given the extent of research necessary to determine an appropriate objective function, halting criterion and simulation count per increment for the optimizer, one area did remain underexplored - the choice of Steepest Ascent Hill Climbing itself. While I did touch on other Neighbourhood Search variations, one possibility that I did not have time to explore was the use of an Evolutionary Algorithm. In such an algorithm, pairs of high-performing manabases would be randomly shuffled together with small mutations at each increment \cite{bjorke2017deckbuilding}. Since I did not trial this algorithm, I do not know if it would have been able to reach more consistent results from an equivalent number of simulated games. This would be a potential route for experimentation in future versions. 

\subsubsection{Mid-Development Validation and Verification Tests}
Although only one participant in my post-development validation tests openly asked for a slower and more accurate product, all users worked more slowly than LandFill. In retrospect, I would have benefitted from a mid-development test after developing the Simulator, where users simply created a manabase on a stopwatch, as this would have enabled me to make more informed decisions about what constitutes a reasonable time for the optimizer to run.

I also did not concudct sufficient verification tests of the Simulator. During development I tested each new land cycle with many sample hands and checked their behaviour, including in combination with other cycles. However, I did not formally Unit Test the Simulator. Unit Testing the Simulator is complicated, as the gameplay decisions of each sample hand play out over many turns. Moreover, as the Simulator chooses lands to play by progressively shrinking a list of possible lands, there is a stochastic element involved if mure than one land is returned by the end of this process, although there are methods to test code with stochastic outputs. Unit testing the Simulator would be an essential test before full deployment, to ensure that existing code is not broken by the introduction of new cycles.

\subsubsection{Time Management During Development}
In retrospect, I may have been able to deliver a more feature-heavy and comprehensively tested/debugged version of LandFill had I allocated my time differently during development. Although my development of the Database was partially a learning exercise in Full Stack Development and Python Coding, skills which facilitated later development, it took up a disproportionate amount of development time. Since I created it before deciding to limit my focus to Commander, and before realizing that choosing playable card faces at random during simulation was an acceptable heuristic, it models many aspects of the game that are not reflected elsewhere in LandFill (although modelling cards and faces seperately is still relevant, as it established my criteria for what a land card is). This does put me in a solid position for future development, however, as modelling multiple-faced spell cards and adding other formats are both potential features to add. 

I had not anticipated, when commencing work on the Simulator, that the question of whether a Lump is playable with a given set of lands would prove as complex as it was. Although I quickly identified all solutions set out in \ref{sec:lumpplayability}, I initially shied away from fully exploring the Linear Assignment-based solution, as initial profiling tests indicated that it ran more slowly for a three colour deck than the memoization option. When the memoization option created difficulties for decks with more colours, I spent too long trying to solve those difficulties, rather than switching to the approach I ended up using. Had I made this decision earlier, I would have had more time to develop the other Components and make more thoughtful decisions in their design.

\subsection{Final Thoughts}
LandFill is a very promising deckbuilder's tool with potential market applicability. Ultimately, it is more successful as an automatic manabase generator for casual use than a comprehensive optimizer, but this is still a valid use case. It may be immediately improved via more thorough Unit Testing and debugging, and implementing more complex simulation code to avoid some of the heuristics outlined in \ref{sec:simulationheuristics}. Broader improvements may be achieved by finding ways to let it conduct more simulated games, either by making the Simulator faster, finding a more efficient Combinatorial Optimization algorithmm or by offering users more input over how long they are content to wait for it. It is, in sum, a successful prototype for an ambitious deckbuilder support app. 

%\bibliographystyle{plain} % We choose the "plain" reference style
%\bibliography{bibliography} % Entries are in the refs.bib file
\printbibliography



\end{document}